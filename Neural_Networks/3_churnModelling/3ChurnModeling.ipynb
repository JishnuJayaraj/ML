{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3ChurnModeling",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTwFUTUE41eZ",
        "colab_type": "code",
        "outputId": "6b39e1fe-4c1f-4f7d-90c7-39bb534b5389",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "# https://medium.com/@pushkarmandot/build-your-first-deep-learning-neural-network-model-using-keras-in-python-a90b5864116d\n",
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "## Importing the dataset\n",
        "# view raw in github and copy apstethe link\n",
        "url = 'https://raw.githubusercontent.com/JishnuJayaraj/PA/master/Batcave/NN/Keras/3_churnModelling/Churn_Modelling.csv'\n",
        "dataset = pd.read_csv(url)\n",
        "\n",
        "# matrix of features and matrix of target variable.\n",
        "X = dataset.iloc[:, 3:13].values\n",
        "y = dataset.iloc[:, 13].values\n",
        "\n",
        "print(X)\n",
        "\n",
        "'''\n",
        "  To dwnld a data frame\n",
        "from google.colab import files\n",
        "\n",
        "df.to_csv('df.csv')\n",
        "files.download('df.csv')\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[619 'France' 'Female' ... 1 1 101348.88]\n",
            " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
            " [502 'France' 'Female' ... 1 0 113931.57]\n",
            " ...\n",
            " [709 'France' 'Female' ... 0 1 42085.58]\n",
            " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
            " [792 'France' 'Female' ... 1 0 38190.78]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n  To dwnld a data frame\\nfrom google.colab import files\\n\\ndf.to_csv('df.csv')\\nfiles.download('df.csv')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0xh48e0Cuh3",
        "colab_type": "text"
      },
      "source": [
        " Let’s make analysis simpler by encoding string variables. Country has string labels such as “France, Spain, Germany” while Gender has “Male, Female”. We have to encode this strings into numeric and we can simply do it using pandas but here I am introducing new library called ‘ScikitLearn’ which is strongest machine learning library in python. We will use ‘LabelEncoder’. As the name suggests, whenever we pass a variable to this function, this function will automatically encode different labels in that column with values between 0 to n_classes-1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPeC3FiyC3nr",
        "colab_type": "code",
        "outputId": "b9c33466-5300-4fd3-8c9e-20f7db96771f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
        "labelencoder_X_2 = LabelEncoder()\n",
        "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
        "\n",
        "# Country names are replaced by 0,1 and 2 while male and female are replaced by 0 and 1.\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[619 0 0 ... 1 1 101348.88]\n",
            " [608 2 0 ... 0 1 112542.58]\n",
            " [502 0 0 ... 1 0 113931.57]\n",
            " ...\n",
            " [709 0 0 ... 0 1 42085.58]\n",
            " [772 1 1 ... 1 0 92888.52]\n",
            " [792 0 0 ... 1 0 38190.78]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAuA8IZDEj3F",
        "colab_type": "text"
      },
      "source": [
        "Label encoding has introduced new problem in our data. LabelEncoder has replaced France with 0, Germany 1 and Spain 2 but Germany is not higher than France and France is not smaller than Spain so we need to create a dummy variable for Country. Dummy variable is difficult concept if you read in depth but don’t take tension, I have found this simple resource which will help you in understanding. We don’t need to do same for Gender Variable as it is binary.\n",
        "\n",
        "    Step 4: How to create dummy variable in python? We will use the same ScikitLearn library but this time we will use another function called as ‘OneHotEncoder’, yeah it is seriously hot. We just need to pass the column number and whoosh your dummy variable is created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Uo7ha_KEoy_",
        "colab_type": "code",
        "outputId": "d007b608-fdf9-4883-aa72-0aedb3ddf2f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
        "X = onehotencoder.fit_transform(X).toarray()\n",
        "X = X[:, 1:]\n",
        "print(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.0000000e+00 0.0000000e+00 6.1900000e+02 ... 1.0000000e+00\n",
            "  1.0000000e+00 1.0134888e+05]\n",
            " [1.0000000e+00 0.0000000e+00 6.0800000e+02 ... 0.0000000e+00\n",
            "  1.0000000e+00 1.1254258e+05]\n",
            " [0.0000000e+00 0.0000000e+00 5.0200000e+02 ... 1.0000000e+00\n",
            "  0.0000000e+00 1.1393157e+05]\n",
            " ...\n",
            " [0.0000000e+00 0.0000000e+00 7.0900000e+02 ... 0.0000000e+00\n",
            "  1.0000000e+00 4.2085580e+04]\n",
            " [0.0000000e+00 1.0000000e+00 7.7200000e+02 ... 1.0000000e+00\n",
            "  0.0000000e+00 9.2888520e+04]\n",
            " [0.0000000e+00 0.0000000e+00 7.9200000e+02 ... 1.0000000e+00\n",
            "  0.0000000e+00 3.8190780e+04]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
            "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
            "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
            "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWTd9HysFYlO",
        "colab_type": "text"
      },
      "source": [
        "In Machine Learning, we always divide our data into training and testing part meaning that we train our model on training data and then we check the accuracy of a model on testing data. Testing your model on testing data will only help you evaluate the efficiency of model.\n",
        "\n",
        "    Step 5: We will make use of ScikitLearn’s ‘train_test_split’ function to divide our data. Roughly people keep 80:20, 75:25, 60:40 as their train test split ratio. Here we are keeping it as 80:20."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RaipvNgFbai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc9JthlXFug2",
        "colab_type": "text"
      },
      "source": [
        " If you carefully observe data, you will find that data is not scaled properly. Some variable has value in thousands while some have value is tens or ones. We don’t want any of our variable to dominate on other so let’s go and scale data.\n",
        "\n",
        "    Step 6: ‘StandardScaler’ is available in ScikitLearn. In the following code we are fitting and transforming StandardScaler method on train data. We have to standardize our scaling so we will use the same fitted method to transform/scale test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvyhWPuiFyjN",
        "colab_type": "code",
        "outputId": "2ce52bf8-edd5-41e9-c863-d6f057ba6c26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "print(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.56945944  1.7194414   1.75588546 ...  0.64395441  0.96922337\n",
            "  -0.608764  ]\n",
            " [-0.56945944 -0.58158423  1.08240097 ...  0.64395441  0.96922337\n",
            "  -0.11393893]\n",
            " [ 1.7560513  -0.58158423 -0.92769122 ...  0.64395441 -1.03175391\n",
            "  -0.24073125]\n",
            " ...\n",
            " [-0.56945944 -0.58158423 -0.24384542 ...  0.64395441  0.96922337\n",
            "   1.43099961]\n",
            " [-0.56945944  1.7194414  -1.45611751 ...  0.64395441  0.96922337\n",
            "  -1.05245766]\n",
            " [-0.56945944  1.7194414   1.96311146 ...  0.64395441 -1.03175391\n",
            "  -0.24544066]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz0x4P8vGG9U",
        "colab_type": "text"
      },
      "source": [
        "Step 7: Importing required Modules. We need Sequential module for initializing NN and dense module to add Hidden Layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7frMLrHGIGq",
        "colab_type": "code",
        "outputId": "1b244282-1b84-42a4-ad94-32aed678053a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Importing the Keras libraries and packages\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQylRvpHG2qc",
        "colab_type": "text"
      },
      "source": [
        " name of model as Classifier as our business problem is the classification of customer churn. In the last step, I mentioned that we will use Sequential module for initialization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvZvlbVEG3k2",
        "colab_type": "code",
        "outputId": "cd0f7819-1a5e-47e8-d0a3-93babf7720aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "#Initializing Neural Network\n",
        "classifier = Sequential()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0627 08:46:59.543601 140059513890688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAq-e0LaHART",
        "colab_type": "text"
      },
      "source": [
        ". We will add hidden layers one by one using dense function.\n",
        "\n",
        "Our first parameter is output_dim. It is simply the number of nodes you want to add to this layer. init is the initialization of Stochastic Gradient Decent. In Neural Network we need to assign weights to each mode which is nothing but importance of that node. At the time of initialization, weights should be close to 0 and we will randomly initialize weights using uniform function. input_dim parameter is needed only for first layer as model doesn’t know the number of our input variables. Remember in our case, the total number of input variables are 11. In the second layer model automatically knows the number of input variable from the first hidden layer.\n",
        "\n",
        "Activation Function: Very important to understand. Neuron applies activation function to weighted sum(summation of Wi * Xi where w is weight, X is input variable and i is suffix of W and X). The closer the activation function value to 1 the more activated is the neuron and more the neuron passes the signal. Which activation function should be used is critical task. Here we are using rectifier(relu) function in our hidden layer and Sigmoid function in our output layer as we want binary result from output layer but if the number of categories in output layer is more than 2 then use SoftMax function.\n",
        "\n",
        "Note: Explaining which output function to use is beyond the scope of this blog, if you have any questions then please leave the comment and I will get back to you asap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnw8-ddNHG6k",
        "colab_type": "code",
        "outputId": "374a9038-855e-4944-9fe1-44e2ec354d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Adding the input layer and the first hidden layer\n",
        "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
        "\n",
        "# Adding the second hidden layer\n",
        "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
        "\n",
        "# Adding the output layer\n",
        "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "W0627 08:47:03.761194 140059513890688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0627 08:47:03.772799 140059513890688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df-Slqk8HqQl",
        "colab_type": "text"
      },
      "source": [
        "    Till now we have added multiple layers to out classifier now let’s compile them which can be done using compile method. Arguments added in final compilation will control whole neural network so be careful on this step. I will briefly explain arguments.\n",
        "\n",
        "First argument is Optimizer, this is nothing but the algorithm you wanna use to find optimal set of weights(Note that in step 9 we just initialized weights now we are applying some sort of algorithm which will optimize weights in turn making out neural network more powerful. This algorithm is Stochastic Gradient descent(SGD). Among several types of SGD algorithm the one which we will use is ‘Adam’. If you go in deeper detail of SGD, you will find that SGD depends on loss thus our second parameter is loss. Since out dependent variable is binary, we will have to use logarithmic loss function called ‘binary_crossentropy’, if our dependent variable has more than 2 categories in output then use ‘categorical_crossentropy’. We want to improve performance of our neural network based on accuracy so add metrics as accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aun7NBR6Hr9M",
        "colab_type": "code",
        "outputId": "4c201279-b3ae-484b-a4a1-c0a81e32c64e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "# Compiling Neural Network\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0627 08:48:21.460162 140059513890688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0627 08:48:21.496868 140059513890688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0627 08:48:21.504124 140059513890688 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4fgIrDsIAHk",
        "colab_type": "text"
      },
      "source": [
        "Step 11: We will now train our model on training data but still one thing is remaining. We use fit method to the fit our model In previous some steps I said that we will be optimizing our weights to improve model efficiency so when are we updating out weights? Batch size is used to specify the number of observation after which you want to update weight. Epoch is nothing but the total number of iterations. Choosing the value of batch size and epoch is trial and error there is no specific rule for that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEKbLUxeIB1D",
        "colab_type": "code",
        "outputId": "a20f3438-eea4-47bc-c6ed-770f973170ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Fitting our model \n",
        "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "W0627 08:49:38.729186 140059513890688 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 2s 210us/step - loss: 0.4973 - acc: 0.7976\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 1s 118us/step - loss: 0.3951 - acc: 0.8371\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3610 - acc: 0.8515\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3506 - acc: 0.8544\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3475 - acc: 0.8561\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3462 - acc: 0.8545\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3456 - acc: 0.8554\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3446 - acc: 0.8567\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3434 - acc: 0.8569\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3435 - acc: 0.8577\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3430 - acc: 0.8580\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3428 - acc: 0.8559\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3426 - acc: 0.8576\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3414 - acc: 0.8581\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3418 - acc: 0.8576\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3411 - acc: 0.8577\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3408 - acc: 0.8589\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3405 - acc: 0.8595\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3404 - acc: 0.8580\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3396 - acc: 0.8611\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3393 - acc: 0.8572\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3393 - acc: 0.8592\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3400 - acc: 0.8594\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3394 - acc: 0.8592\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 1s 110us/step - loss: 0.3394 - acc: 0.8587\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3392 - acc: 0.8586\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3389 - acc: 0.8590\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3388 - acc: 0.8589\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3387 - acc: 0.8612\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3382 - acc: 0.8599\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3386 - acc: 0.8580\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3383 - acc: 0.8605\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3381 - acc: 0.8586\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3381 - acc: 0.8579\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3374 - acc: 0.8601\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3376 - acc: 0.8596\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3375 - acc: 0.8601\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3372 - acc: 0.8589\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3374 - acc: 0.8602\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3366 - acc: 0.8607\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3372 - acc: 0.8605\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3367 - acc: 0.8604\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3367 - acc: 0.8596\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3367 - acc: 0.8589\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3363 - acc: 0.8597\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3364 - acc: 0.8577\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3367 - acc: 0.8600\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 1s 116us/step - loss: 0.3363 - acc: 0.8612\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3363 - acc: 0.8610\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3362 - acc: 0.8601\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3358 - acc: 0.8600\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3366 - acc: 0.8590\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3362 - acc: 0.8609\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3364 - acc: 0.8589\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3364 - acc: 0.8600\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3357 - acc: 0.8615\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 1s 116us/step - loss: 0.3363 - acc: 0.8589\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 1s 118us/step - loss: 0.3357 - acc: 0.8607\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 1s 123us/step - loss: 0.3362 - acc: 0.8601\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3361 - acc: 0.8605\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3359 - acc: 0.8620\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3359 - acc: 0.8604\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3363 - acc: 0.8610\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3360 - acc: 0.8606\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3356 - acc: 0.8607\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3358 - acc: 0.8599\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3361 - acc: 0.8596\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3358 - acc: 0.8606\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3356 - acc: 0.8611\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3353 - acc: 0.8586\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3354 - acc: 0.8604\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3357 - acc: 0.8596\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3358 - acc: 0.8594\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3356 - acc: 0.8595\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3350 - acc: 0.8605\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3354 - acc: 0.8599\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3354 - acc: 0.8610\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3358 - acc: 0.8595\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3351 - acc: 0.8599\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3355 - acc: 0.8615\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3353 - acc: 0.8611\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3349 - acc: 0.8596\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3351 - acc: 0.8585\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3356 - acc: 0.8591\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3352 - acc: 0.8604\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3344 - acc: 0.8626\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3349 - acc: 0.8605\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3347 - acc: 0.8581\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 1s 112us/step - loss: 0.3348 - acc: 0.8602\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3349 - acc: 0.8614\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3350 - acc: 0.8622\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3353 - acc: 0.8600\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 1s 117us/step - loss: 0.3349 - acc: 0.8596\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3348 - acc: 0.8607\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 1s 115us/step - loss: 0.3345 - acc: 0.8600\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 1s 111us/step - loss: 0.3347 - acc: 0.8601\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3349 - acc: 0.8607\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 1s 114us/step - loss: 0.3351 - acc: 0.8602\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 1s 113us/step - loss: 0.3348 - acc: 0.8599\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 1s 118us/step - loss: 0.3342 - acc: 0.8600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f61e4d4feb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU3H_bFiIdmZ",
        "colab_type": "text"
      },
      "source": [
        "Step 12: Predicting the test set result. The prediction result will give you probability of the customer leaving the company. We will convert that probability into binary 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqhjqSzdIej8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hglIo8opIlWY",
        "colab_type": "text"
      },
      "source": [
        "Step 13: This is the final step where we are evaluating our model performance. We already have original results and thus we can build confusion matrix to check the accuracy of model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCsTzpc7ImPz",
        "colab_type": "code",
        "outputId": "a7754733-4312-4553-ab69-e1f1ea251006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "cm# Creating the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1531   71]\n",
            " [ 196  202]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "799lt2GTZHFW",
        "colab_type": "code",
        "outputId": "5bc133f4-0eec-4a5a-dba3-565848ca14e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2200.000\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf\n",
            "bogomips\t: 4400.00\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqoBuAwTI05f",
        "colab_type": "text"
      },
      "source": [
        "So the Accuracy of our model can be calculated as:\n",
        "\n",
        "Accuracy= 1550+175/2000=0.8625\n",
        "\n",
        "Awesome, we achieved 86.25% accuracy which is quite good."
      ]
    }
  ]
}