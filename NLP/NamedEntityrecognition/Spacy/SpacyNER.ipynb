{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpacyNER.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "19snhtebKD0JJN6HsvDye9xHWrV2x_uOK",
      "authorship_tag": "ABX9TyP5438auA7+5m0dHqHb6f4U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JishnuJayaraj/ML/blob/master/NLP/NamedEntityrecognition/Spacy/SpacyNER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWUer8C6h9TN"
      },
      "source": [
        "# SPACY NER TRAINING\n",
        "\n",
        "[sample code](https://towardsdatascience.com/custom-named-entity-recognition-using-spacy-7140ebbb3718)\n",
        "\n",
        "[sample tutorial](https://lvngd.com/blog/how-to-train-a-custom-named-entity-recognizer-with-spacy/)\n",
        "\n",
        "[sample 2](https://medium.com/swlh/python-nlp-tutorial-information-extraction-and-knowledge-graphs-43a2a4c4556c)\n",
        "[spacy documentation](https://spacy.io/usage/training)\n",
        "\n",
        "To extract named entities, you pass a piece of text to the NER model and it looks at each word and tries to predict whether the word fits into a named entity category such as person, location, organization, etc.\n",
        "\n",
        "Problems arise when the text data you're trying to label is too different(yes, very subjective) than the text data that was used to train the Named Entity Recognizer you're using, and it might not be very good at labeling your data.\n",
        "\n",
        "### Training data in JSON format\n",
        "TRAIN_DATA = [\n",
        "        (\"Uber blew through $1 million a week\", \n",
        "                    {\"entities\": \n",
        "                        [(0, 4, \"ORG\")]\n",
        "                        }\n",
        "                    ),\n",
        "        (\"Google rebrands its business apps\", \n",
        "                    {\"entities\": \n",
        "                        [(0, 6, \"ORG\")]\n",
        "                        }\n",
        "                    )           \n",
        "                ]\n",
        "\n",
        "### save some for test data\n",
        "\n",
        "[sapcy for triplet: kaggle](https://www.kaggle.com/shivamb/spacy-text-meta-features-knowledge-graphs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5ZpNORXHVkf",
        "outputId": "829131e0-bf22-42c9-db0d-56a3507509a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!python -m spacy download en_core_web_md\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (47.3.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34irHl4SmrIK",
        "outputId": "808a822c-3cb2-4c8b-df55-7ee4a11edc0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUm-OUB-I-Io"
      },
      "source": [
        "# load data\n",
        "orig = pd.read_json('/content/drive/My Drive/RokinData/ToBeCleaned.json.gz')\n",
        "orig['word_count'] = orig['text'].apply(lambda x: len(str(x).split(\" \")))\n",
        "orig['content_len'] = orig['text'].astype(str).apply(len)\n",
        "orig.sample(5)\n",
        "df = orig.sample(100000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qql4sI1LLi9h"
      },
      "source": [
        "# Language detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGwUwzAlT9KV"
      },
      "source": [
        "!pip install spacy_cld\n",
        "import spacy\n",
        "from spacy_cld import LanguageDetector\n",
        "nlp = spacy.load('en')\n",
        "language_detector = LanguageDetector()\n",
        "nlp.add_pipe(language_detector)\n",
        "\n",
        "#sample\n",
        "doc = nlp('This is some English text.')\n",
        "doc._.languages  # ['en']\n",
        "doc._.language_scores['en']  # 0.96\n",
        "\n",
        "tweets          = df['text']\n",
        "languages_spacy = []\n",
        "\n",
        "for e in tweets:\n",
        "    doc = nlp(e)\n",
        "    # cheking if the doc._.languages is not empty\n",
        "    # then appending the first detected language in a list\n",
        "    if(doc._.languages):\n",
        "        languages_spacy.append(doc._.languages[0])\n",
        "    # if it is empty, we append the list by unknown\n",
        "    else:\n",
        "        languages_spacy.append('unknown')\n",
        "\n",
        "df['languages_spacy'] = languages_spacy\n",
        "#df['languages_langdetect'] = languages_langdetect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaOJWfu-asGA"
      },
      "source": [
        "df['languages_spacy'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9UxsxjeIQPj",
        "outputId": "730e8c4b-a28e-40a4-8017-afac9ad62b00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "de=df.loc[df['languages_spacy'] == 'sk']\n",
        "de.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>url</th>\n",
              "      <th>lastCrawlTimeUTC</th>\n",
              "      <th>word_count</th>\n",
              "      <th>content_len</th>\n",
              "      <th>languages_spacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>399616</th>\n",
              "      <td>1575432000000</td>\n",
              "      <td>Na trh východnej Európy prichádza kompletný so...</td>\n",
              "      <td>GUANGZHOU, Čína, 4. decembra 2019 /PRNewswire/...</td>\n",
              "      <td>https://www.prnewswire.com/news-releases/na-tr...</td>\n",
              "      <td>1589863905</td>\n",
              "      <td>334</td>\n",
              "      <td>2416</td>\n",
              "      <td>sk</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 date  ... languages_spacy\n",
              "399616  1575432000000  ...              sk\n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FdYJEgTmViD"
      },
      "source": [
        "# term freq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76VrFixj0MW3"
      },
      "source": [
        "## unigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSLSuwBqma0J"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops =  set(stopwords.words('english')+['com'])\n",
        "co = CountVectorizer(stop_words=stops)\n",
        "counts = co.fit_transform(df.text)\n",
        "table1= pd.DataFrame(counts.sum(axis=0),columns=co.get_feature_names()).T.sort_values(0,ascending=False).head(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr6NG2-knqnE"
      },
      "source": [
        "# most frequent words in the data, extracting information about its content and topics.\n",
        "\n",
        "# bar chart with custom regex\n",
        "# data.Tweet_Text.str.extractall(r'(\\#\\w+)')[0].value_counts().head(20).plot.bar()\n",
        "table1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZM4P3gi0QGk"
      },
      "source": [
        "## Bigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-jtJgtqoGVh"
      },
      "source": [
        "co = CountVectorizer(ngram_range=(2,2),stop_words=stops)\n",
        "counts = co.fit_transform(df.text)\n",
        "table2=pd.DataFrame(counts.sum(axis=0),columns=co.get_feature_names()).T.sort_values(0,ascending=False).head(50)\n",
        "table2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddl1nIPN0UnG"
      },
      "source": [
        "## topic modelling LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHnbEOotpRte"
      },
      "source": [
        "#LDA\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "vectorizer = CountVectorizer(stop_words=stops)\n",
        "model = vectorizer.fit(df.text)\n",
        "docs = vectorizer.transform(df.text)\n",
        "lda = LatentDirichletAllocation(20)\n",
        "lda.fit(docs)\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "  for topic_idx, topic in enumerate(model.components_):\n",
        "    message = \"Topic #%d: \" % topic_idx\n",
        "    message += \" \".join([(feature_names[i]) for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "    print(message)\n",
        "  print()\n",
        "print_top_words(lda,vectorizer.get_feature_names(),10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHpDn6wO0ZQk"
      },
      "source": [
        "## LDA2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqGssGEYtq7y",
        "outputId": "cbe88683-e539-4095-8f0f-6541d8aaad11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "#LDA 2 \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "df['tokenized'] = df['text'].apply(word_tokenize)\n",
        "df['lower'] = df['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
        "\n",
        "import string\n",
        "punc = string.punctuation\n",
        "df['no_punc'] = df['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['stopwords_removed'] = df['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "df['pos_tags'] = df['stopwords_removed'].apply(nltk.tag.pos_tag)\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "df['wordnet_pos'] = df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "df['lemmatized'] = df['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
        "\n",
        "df['lemma_str'] = [' '.join(map(str,l)) for l in df['lemmatized']]\n",
        "\n",
        "# add df.drop()\n",
        "\n",
        "\n",
        "tf_vectorizer = CountVectorizer(max_df=0.9, min_df=25, max_features=5000)\n",
        "tf = tf_vectorizer.fit_transform(df['lemma_str'].values.astype('U'))\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "doc_term_matrix = pd.DataFrame(tf.toarray(), columns=list(tf_feature_names))\n",
        "\n",
        "lda_model = LatentDirichletAllocation(n_components=10, learning_method='online', max_iter=500, random_state=0).fit(tf)\n",
        "no_top_words = 10\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([feature_names[i]\n",
        "                          for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "              \n",
        "display_topics(lda_model, tf_feature_names, no_top_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Topic 0:\n",
            "use data system software user application device network support design\n",
            "Topic 1:\n",
            "de die la und der en el für que para\n",
            "Topic 2:\n",
            "use say material researcher research new university could light make\n",
            "Topic 3:\n",
            "technology company customer product solution service business industry system new\n",
            "Topic 4:\n",
            "year company say robot new percent business 000 manufacturing employee\n",
            "Topic 5:\n",
            "say make one get use like work see time go\n",
            "Topic 6:\n",
            "market company million 2019 report table global 2020 share growth\n",
            "Topic 7:\n",
            "design machine part material tool use 3d new metal company\n",
            "Topic 8:\n",
            "power high system application sensor use design device low range\n",
            "Topic 9:\n",
            "system time process need use cost change work control equipment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7IIl7fv0dcK"
      },
      "source": [
        "### LDA vis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXivKhhiwrLQ"
      },
      "source": [
        "!pip install pyldavis\n",
        "import pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn.prepare(lda_model, tf, tf_vectorizer, mds='tsne')\n",
        "panel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neJsz3dJw-xl"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0M1ETHmPxBN_"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df =25, max_features=5000, use_idf=True)\n",
        "tfidf = tfidf_vectorizer.fit_transform(df['lemma_str'])\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "doc_term_matrix_tfidf = pd.DataFrame(tfidf.toarray(), columns=list(tfidf_feature_names))\n",
        "\n",
        "#NMF\n",
        "nmf = NMF(n_components=10, random_state=0, alpha=.1, init='nndsvd').fit(tfidf)\n",
        "display_topics(nmf, tfidf_feature_names, no_top_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRHP-0mEmHab"
      },
      "source": [
        "# NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTM5Yw_PGB3I"
      },
      "source": [
        "https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
        "\n",
        "preprocessing\n",
        "[simpple text processing](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiUGA9lrHgaY"
      },
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7tPeTCMHpLD"
      },
      "source": [
        "# Sample\n",
        "doc=nlp('India and Iran have agreed to boost the economic viability \\\n",
        "of the strategic Chabahar port through various measures, \\\n",
        "including larger subsidies to merchant shipping firms using the facility, \\\n",
        "people familiar with the development said on Thursday.')\n",
        "\n",
        "[(x.text,x.label_) for x in doc.ents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FNEiZ-OIrQK"
      },
      "source": [
        "![alt text](https://i2.wp.com/neptune.ai/wp-content/uploads/spacy_ner.png?w=647&ssl=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSH7bBjoHvbD"
      },
      "source": [
        "#sample visualization\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLcvO_t8I6n8"
      },
      "source": [
        "# Run NER on our data\n",
        "from collections import Counter \n",
        "\n",
        "df = orig.head(10000)\n",
        "def ner(text):\n",
        "    doc=nlp(text)\n",
        "    return [X.label_ for X in doc.ents]\n",
        "\n",
        "ent=df['text'].apply(lambda x : ner(x))\n",
        "ent=[x for sub in ent for x in sub]\n",
        "\n",
        "counter=Counter(ent)\n",
        "count=counter.most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFVM9QRAJQUq"
      },
      "source": [
        "import seaborn as sns\n",
        "# visualize entity freq\n",
        "x,y=map(list,zip(*count))\n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIJl-A7uJaGW"
      },
      "source": [
        "# check which specified entity occur the most\n",
        "def ner(text,ent=\"ORG\"):\n",
        "    doc=nlp(text)\n",
        "    return [X.text for X in doc.ents if X.label_ == ent]\n",
        "\n",
        "gpe=df['text'].apply(lambda x: ner(x))\n",
        "gpe=[i for x in gpe for i in x]\n",
        "counter=Counter(gpe)\n",
        "\n",
        "x,y=map(list,zip(*counter.most_common(10)))\n",
        "sns.barplot(y,x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v20j_3TWKDLO"
      },
      "source": [
        "# person\n",
        "per=df['text'].apply(lambda x: ner(x,\"PERSON\"))\n",
        "per=[i for x in per for i in x]\n",
        "counter=Counter(per)\n",
        "\n",
        "x,y=map(list,zip(*counter.most_common(10)))\n",
        "sns.barplot(y,x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWInvb2Dc7BJ"
      },
      "source": [
        "# many product might go u noticed\n",
        "per=df['text'].apply(lambda x: ner(x,\"PRODUCT\"))\n",
        "per=[i for x in per for i in x]\n",
        "counter=Counter(per)\n",
        "\n",
        "x,y=map(list,zip(*counter.most_common(10)))\n",
        "sns.barplot(y,x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AddpGyqtjJLm"
      },
      "source": [
        "# POS tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3rj7Gb8jLoX"
      },
      "source": [
        "Noun (NN)- Joseph, London, table, cat, teacher, pen, city\n",
        "\n",
        "Verb (VB)- read, speak, run, eat, play, live, walk, have, like, are, is\n",
        "\n",
        "Adjective(JJ)- beautiful, happy, sad, young, fun, three\n",
        "\n",
        "Adverb(RB)- slowly, quietly, very, always, never, too, well, tomorrow\n",
        "\n",
        "Preposition (IN)- at, on, in, from, with, near, between, about, under\n",
        "\n",
        "Conjunction (CC)- and, or, but, because, so, yet, unless, since, if\n",
        "\n",
        "Pronoun(PRP)- I, you, we, they, he, she, it, me, us, them, him, her, this\n",
        "\n",
        "Interjection (INT)- Ouch! Wow! Great! Help! Oh! Hey! Hi!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6zaxbqMjqdZ"
      },
      "source": [
        "doc = nlp('The greatest comeback stories in 2019')\n",
        "displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yffmtOvlkKpU"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgPX5AgHkDIp"
      },
      "source": [
        "# Run POS on title\n",
        "def pos(text):\n",
        "    pos=nltk.pos_tag(word_tokenize(text))\n",
        "    pos=list(map(list,zip(*pos)))[1]\n",
        "    return pos\n",
        "\n",
        "tags=df['title'].apply(lambda x : pos(x))\n",
        "tags=[x for l in tags for x in l]\n",
        "counter=Counter(tags)\n",
        "\n",
        "x,y=list(map(list,zip(*counter.most_common(7))))\n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byYtSMVjk0CS"
      },
      "source": [
        "# which singular noun dominates\n",
        "def get_adjs(text):\n",
        "    adj=[]\n",
        "    pos=nltk.pos_tag(word_tokenize(text))\n",
        "    for word,tag in pos:\n",
        "        if tag=='NN':\n",
        "            adj.append(word)\n",
        "    return adj\n",
        "\n",
        "\n",
        "words=df['title'].apply(lambda x : get_adjs(x))\n",
        "words=[x for l in words for x in l]\n",
        "counter=Counter(words)\n",
        "\n",
        "x,y=list(map(list,zip(*counter.most_common(7))))\n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5ps1WFIRRk7"
      },
      "source": [
        "## Person name detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v83hfzg7P-V3"
      },
      "source": [
        "# Extracting names usig spacy\n",
        "!pip3 install spacy\n",
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "!pip install dateparser\n",
        "!python3 -m spacy download en_core_web_md\n",
        "!python -m spacy download en_core_web_sm\n",
        "import en_core_web_md\n",
        "import dateparser\n",
        "\n",
        "def expand_person_entities(doc):\n",
        "    new_ents = []\n",
        "    for ent in doc.ents:\n",
        "        # Only check for title if it's a person and not the first token\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            if ent.start != 0:\n",
        "                # if person preceded by title, include title in entity\n",
        "                prev_token = doc[ent.start - 1]\n",
        "                if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
        "                    new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
        "                    new_ents.append(new_ent)\n",
        "                else:\n",
        "                    # if entity can be parsed as a date, it's not a person\n",
        "                    if dateparser.parse(ent.text) is None:\n",
        "                        new_ents.append(ent) \n",
        "        else:\n",
        "            new_ents.append(ent)\n",
        "    doc.ents = new_ents\n",
        "    return doc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Add the component after the named entity recognizer\n",
        "# nlp.remove_pipe('expand_person_entities')\n",
        "nlp = spacy.load('en_core_web_md') #nlp = spacy.load('en') #nlp = en_core_web_md.load()  #nlp = spacy.load('en_core_web_md')  \n",
        "nlp.add_pipe(expand_person_entities, after='ner')\n",
        "\n",
        "document_string = 'Jishnu jayaraj was a great person and still is. Mr. Jayaraj was well known for good deeds. Dr. Jishnu Jayaraj and team worked good but Dr. Jayden Green Olivia team failed so bad to him  '\n",
        "doc = nlp(document_string)\n",
        "[(ent.text, ent.label_) for ent in text.ents if ent.label_=='PERSON']\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}