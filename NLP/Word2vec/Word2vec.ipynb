{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMw4YsqEkWZLvDjU1tJ7H2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JishnuJayaraj/ML/blob/master/NLP/Word2vec/Word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3TB2__698k2"
      },
      "source": [
        "# Word2Vec\n",
        "\n",
        "[link text](https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgWDD0phvawY"
      },
      "source": [
        "## Cleaning samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRwbrB4lvdHz"
      },
      "source": [
        "clean_txt = []\n",
        "for w in range(len(df.text)):\n",
        "    desc = df['text'][w].lower()\n",
        "    \n",
        "    #remove punctuation\n",
        "    desc = re.sub('[^a-zA-Z]', ' ', desc)\n",
        "    \n",
        "    #remove tags  change this!!!\n",
        "    desc=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",desc)\n",
        "    \n",
        "    #remove digits and special chars\n",
        "    desc=re.sub(\"(\\\\d|\\\\W)+\",\" \",desc)\n",
        "    clean_txt.append(desc)\n",
        "df['clean'] = clean_txt\n",
        "df.head()\n",
        "\n",
        "# tokenization\n",
        "corpus = []\n",
        "for col in df.clean:\n",
        "    word_list = col.split(\" \")\n",
        "    corpus.append(word_list)\n",
        "#show first value\n",
        "corpus[0:1]\n",
        "\n",
        "\n",
        "## -----------------------------------------------------------------------\n",
        "# http://ethen8181.github.io/machine-learning/deep_learning/word2vec/word2vec_detailed.html\n",
        "\n",
        "\n",
        "def export_unigrams(unigram_path, texts, stop_words):\n",
        "    \"\"\"\n",
        "    Preprocessed the raw text and export it to a .txt file,\n",
        "    where each line is one document, for what sort of preprocessing\n",
        "    is done, please refer to the `normalize_text` function\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unigram_path : str\n",
        "        output file path of the preprocessed unigram text.\n",
        "\n",
        "    texts : iterable\n",
        "        iterable can be simply a list, but for larger corpora,\n",
        "        consider an iterable that streams the sentences directly from\n",
        "        disk/network using Gensim's Linsentence or something along\n",
        "        those line.\n",
        "\n",
        "    stop_words : set\n",
        "        stopword set that will be excluded from the corpus.\n",
        "    \"\"\"\n",
        "    with open(unigram_path, 'w', encoding='utf_8') as f:\n",
        "        for text in texts:\n",
        "            cleaned_text = normalize_text(text, stop_words)\n",
        "            f.write(cleaned_text + '\\n')\n",
        "\n",
        "\n",
        "def normalize_text(text, stop_words):\n",
        "    # remove special characters\\whitespaces\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)\n",
        "\n",
        "    # lower case & tokenize text\n",
        "    tokens = re.split(r'\\s+', text.lower().strip())\n",
        "\n",
        "    # filter stopwords out of text &\n",
        "    # re-create text from filtered tokens\n",
        "    cleaned_text = ' '.join(token for token in tokens if token not in stop_words)\n",
        "    return cleaned_text\n",
        "\n",
        "# a set of stopwords built-in to various packages\n",
        "# we can always expand this set for the\n",
        "# problem that we are working on, here we also included\n",
        "# python built-in string punctuation mark\n",
        "STOPWORDS = set(stopwords.words('english')) | set(punctuation) | set(ENGLISH_STOP_WORDS)\n",
        "\n",
        "# create a directory called 'model' to\n",
        "# store all outputs in later section\n",
        "MODEL_DIR = 'model'\n",
        "if not os.path.isdir(MODEL_DIR):\n",
        "    os.mkdir(MODEL_DIR)\n",
        "\n",
        "UNIGRAM_PATH = os.path.join(MODEL_DIR, 'unigram.txt')\n",
        "if not os.path.exists(UNIGRAM_PATH):\n",
        "    start = time()\n",
        "    export_unigrams(UNIGRAM_PATH, texts=newsgroups_train.data, stop_words=STOPWORDS)\n",
        "    elapse = time() - start\n",
        "    print('text preprocessing, elapse', elapse)\n",
        "\n",
        "PHRASE_MODEL_CHECKPOINT = os.path.join(MODEL_DIR, 'phrase_model')\n",
        "if os.path.exists(PHRASE_MODEL_CHECKPOINT):\n",
        "    phrase_model = Phrases.load(PHRASE_MODEL_CHECKPOINT)\n",
        "else:\n",
        "    # use LineSentence to stream text as oppose to\n",
        "    # loading it all into memory\n",
        "    unigram_sentences = LineSentence(UNIGRAM_PATH)\n",
        "    start = time()\n",
        "    phrase_model = Phrases(unigram_sentences)\n",
        "    elapse = time() - start\n",
        "    print('training phrase model, elapse', elapse)\n",
        "    phrase_model.save(PHRASE_MODEL_CHECKPOINT)\n",
        "\n",
        "def export_bigrams(unigram_path, bigram_path, phrase_model):\n",
        "    \"\"\"\n",
        "    Use the learned phrase model to create (potential) bigrams,\n",
        "    and output the text that contains bigrams to disk\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    unigram_path : str\n",
        "        input file path of the preprocessed unigram text\n",
        "\n",
        "    bigram_path : str\n",
        "        output file path of the transformed bigram text\n",
        "\n",
        "    phrase_model : gensim's Phrase model object\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    Gensim Phrase Detection\n",
        "    - https://radimrehurek.com/gensim/models/phrases.html\n",
        "    \"\"\"\n",
        "\n",
        "    # after training the Phrase model, create a performant\n",
        "    # Phraser object to transform any sentence (list of\n",
        "    # token strings) and glue unigrams together into bigrams\n",
        "    phraser = Phraser(phrase_model)\n",
        "    with open(bigram_path, 'w') as fout, open(unigram_path) as fin:\n",
        "        for text in fin:\n",
        "            unigram = text.split()\n",
        "            bigram = phraser[unigram]\n",
        "            bigram_sentence = ' '.join(bigram)\n",
        "            fout.write(bigram_sentence + '\\n')\n",
        "\n",
        "BIGRAM_PATH = os.path.join(MODEL_DIR, 'bigram.txt')\n",
        "if not os.path.exists(BIGRAM_PATH):\n",
        "    start = time()\n",
        "    export_bigrams(UNIGRAM_PATH, BIGRAM_PATH, phrase_model)\n",
        "    elapse = time() - start\n",
        "    print('converting words to phrases, elapse', elapse)\n",
        "\n",
        "word2vec = Word2Vec(corpus_file=BIGRAM_PATH, workers=cpu_count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHNal_ozvegd"
      },
      "source": [
        "## load data from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ2To8N9vjwQ"
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# orig = pd.read_pickle('/content/drive/My Drive/RokinData/newOnly.pkl')\n",
        "orig = pd.read_json('/content/drive/My Drive/RokinData/ToBeCleaned.json.gz')\n",
        "\n",
        "df = orig.sample(50000)\n",
        "del orig\n",
        "\n",
        "print('length of df is :', len(df))\n",
        "df.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i9ws42Lvy5v"
      },
      "source": [
        "## creating custom .txt files from it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XYpXTbxv0ZX"
      },
      "source": [
        "# Sort df based on year-month and save to different txt files\n",
        "\n",
        "dfs = dict(tuple(df.groupby([df['date'].dt.year,df['date'].dt.month])))\n",
        "len(dfs[(2020, 6)])\n",
        "\n",
        "\n",
        "# --------------------------> select month & Year here <------------------------\n",
        "a = dfs[(2020, 6)]\n",
        "\n",
        "# a['text'].replace('\\s+', ' ', regex=True, inplace=True) # remove extra whitespace\n",
        "# a['text'].replace('\\n',' ', regex=True, inplace=True) # remove \\n in text\n",
        "a['text'].replace(r'\\s+|\\n', ' ', regex=True, inplace=True) \n",
        "\n",
        "# a['text'].to_csv(r'/content/drive/My Drive/RokinData/word2vec/3.txt', header=None, index=None, sep=' ', mode='a')\n",
        "with open('/content/drive/My Drive/RokinData/word2vec/output.txt', 'w') as f:\n",
        "    f.write(a['text'].str.cat(sep='\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFQID8GOv2cK"
      },
      "source": [
        "## iterator for folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWymXDLPv5z2"
      },
      "source": [
        "# go thr all files in folder\n",
        "import os\n",
        "\n",
        "class WordTrainer(object):\n",
        "   def __init__(self, dir_name):\n",
        "      self.dir_name = dir_name\n",
        "   def __iter__(self):\n",
        "      for idx,file_name in enumerate(os.listdir(self.dir_name)):   # go thr each files\n",
        "        for idxx,line in enumerate(open(os.path.join(self.dir_name, file_name),'r')):  # open each files\n",
        "            # words = [word.lower() for word in line.split()]\n",
        "            # yield words\n",
        "            tokenized_list = simple_preprocess(line, deacc=True)\n",
        "\n",
        "articles1 = WordTrainer('/content/drive/My Drive/RokinData/word2vec')\n",
        "model = Word2Vec(articles, min_count=1,size= 50,workers=3, window =3, sg = 1)\n",
        "#  word_vector_model = gensim.models.Word2Vec(articles1, size=100, window=8, min_count=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v3pDF5Av8a0"
      },
      "source": [
        "## iterator for file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRBhLofpwAej"
      },
      "source": [
        "# go thr give file\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "class SentenceIterator: \n",
        "    def __init__(self, filepath): \n",
        "        self.filepath = filepath \n",
        "\n",
        "    def __iter__(self): \n",
        "        for line in open(self.filepath): \n",
        "            # yield line.split()     # add code here to make list of list\n",
        "            yield simple_preprocess(line, deacc=True)\n",
        "\n",
        "# define model\n",
        "model = gensim.models.Word2Vec(size=100)\n",
        "\n",
        "sentences = SentenceIterator('/content/drive/My Drive/RokinData/word2vec/sample/1.txt') \n",
        "model.build_vocab(sentences)\n",
        "model.train(sentences,epochs=3,total_examples=model.corpus_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN46ZCCdwEV3"
      },
      "source": [
        "model.wv.most_similar(positive='study')\n",
        "\n",
        "list1 = model.wv.vocab\n",
        "model.wv.vectors.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EjycZjFwGrH"
      },
      "source": [
        "# train 2nd articls set\n",
        "sentences2 = SentenceIterator('/content/drive/My Drive/RokinData/word2vec/sample/2.txt')\n",
        "\n",
        "model.build_vocab(sentences2, update=True)\n",
        "\n",
        "model.train(sentences2,epochs=3,total_examples=model.corpus_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptr6hXxIwIym"
      },
      "source": [
        "list2 = model.wv.vocab\n",
        "model.wv.vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1lVYCiuwL5c"
      },
      "source": [
        "# difference of 2 vocab\n",
        "# def difference(list1,list2):\n",
        "#     return (list(set(list1) - set(list2)))\n",
        "\n",
        "# difference(list1,list2)\n",
        "# list(set(list1) - set(list2))\n",
        "value = { k : list2[k] for k in set(list2) - set(list1) }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdKJDhjXwOwg"
      },
      "source": [
        "# difference of 2 vocab\n",
        "# def difference(list1,list2):\n",
        "#     return (list(set(list1) - set(list2)))\n",
        "\n",
        "# difference(list1,list2)\n",
        "# list(set(list1) - set(list2))\n",
        "value = { k : list2[k] for k in set(list2) - set(list1) }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZN-ZVqQwRhK"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNWAbegqwWUN"
      },
      "source": [
        "import re  # For preprocessing\n",
        "import pandas as pd  # For data handling\n",
        "from time import time  # To time our operations\n",
        "from collections import defaultdict  # For word frequency\n",
        "\n",
        "import spacy  # For preprocessing\n",
        "\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hj2ZavMwaWf"
      },
      "source": [
        "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
        "\n",
        "def cleaning(doc):\n",
        "    # Lemmatizes and removes stopwords\n",
        "    # doc needs to be a spacy Doc object\n",
        "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
        "    # if a sentence is only one or two words long,\n",
        "    # the benefit for the training is very small\n",
        "    if len(txt) > 2:\n",
        "        return ' '.join(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8We0U-u8wcnq"
      },
      "source": [
        "# remove non alphabet\n",
        "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['text'])\n",
        "# [^a-zA-Z0-9\\u00E4\\u00F6\\u00FC\\u00C4\\u00D6\\u00DC\\u00df]   to include geman characters\n",
        "# \\u00F0-\\u02AF             all characters from europian languages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sxq6pcfwe-g"
      },
      "source": [
        "t = time()\n",
        "\n",
        "%time txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=500, n_threads=-1)]\n",
        "\n",
        "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Npa1RRowg4f"
      },
      "source": [
        "#Put the results in a DataFrame to remove missing values and duplicates:\n",
        "df_clean = pd.DataFrame({'clean': txt})\n",
        "df_clean = df_clean.dropna().drop_duplicates()\n",
        "df_clean.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLnFAjhowkXe"
      },
      "source": [
        "del df\n",
        "del txt\n",
        "\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "\n",
        "#As Phrases() takes a list of list of words as input:\n",
        "sent = [row.split() for row in df_clean['clean']]\n",
        "\n",
        "#Creates the relevant phrases from the list of sentences\n",
        "%time phrases = Phrases(sent, min_count=30, progress_per=10000)\n",
        "\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYHesY5OwnFh"
      },
      "source": [
        "# Transform the corpus based on the bigrams detected\n",
        "sentences = phrases[sent]\n",
        "\n",
        "# sentences[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CBmSaEOwrgx"
      },
      "source": [
        "word_freq = defaultdict(int)\n",
        "for sent in sentences:\n",
        "    for i in sent:\n",
        "        word_freq[i] += 1\n",
        "len(word_freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj8DOTYWwty1"
      },
      "source": [
        "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb-4XFeEwxj9"
      },
      "source": [
        "## Training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3283kxpwxK8"
      },
      "source": [
        "# Training Model\n",
        "import multiprocessing\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
        "\n",
        "\n",
        "# add seed= 12345, sg=1 for skip gram,               :: an empty model, no training yet\n",
        "w2v_model = Word2Vec(min_count=20,\n",
        "                     window=2,            # The maximum distance between the target word and its neighboring word\n",
        "                     size=300,            # The size of the dense vector to represent each token, Bigger size values require more training data, but can lead to better (more accurate) models\n",
        "                     sample=6e-5,         # lower value discards more freq ocureing words\n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,         # Negative Sampling: training sample to update only a small percentage of the model's weights\n",
        "                     workers=cores-1)      # How many threads to use"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNCyaup8w1pT"
      },
      "source": [
        "t = time()\n",
        "\n",
        "# Vocab table :simply digesting all the words and filtering out the unique words, and doing some basic counts on them\n",
        "%time w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bsVMsAQw49p"
      },
      "source": [
        "# train model\n",
        "t = time()\n",
        "\n",
        "%time w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPu7RY50w7lN"
      },
      "source": [
        "## Load/ Save model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sYwOVqCxUXX"
      },
      "source": [
        "if you don’t need the full model state any more (don’t need to continue training) then model can be discarded. use keyed vectors instead for faster usage\n",
        "\n",
        "- It is impossible to continue training the vectors loaded from the C format because the hidden weights, vocabulary frequencies and the binary tree are missing. To continue training, you’ll need the full Word2Vec object state, as stored by save(), not just the KeyedVectors.\n",
        "\n",
        "Example code for loading keyed vectors\n",
        "```\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model.wv.save(path)\n",
        "wv = KeyedVectors.load(\"model.wv\", mmap='r')\n",
        "vector = wv['computer']\n",
        "```\n",
        "\n",
        "Example code for loading text/bin format\n",
        "\n",
        "\n",
        "```\n",
        "wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)  # C text format\n",
        "\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(datapath(\"euclidean_vectors.bin\"), binary=True)  # C bin format\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKLYwEozw99o"
      },
      "source": [
        "# -----------> saving\n",
        "# w2v_model.save(\"/content/drive/My Drive/RokinData/models/word2vec_50k.model\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AhTMIHRxLn8"
      },
      "source": [
        "# ------------> loading\n",
        "from gensim.models import Word2Vec\n",
        "w2v_model = Word2Vec.load(\"/content/drive/My Drive/RokinData/models/word2vec_50k.model\")\n",
        "# this model can be used to continue training\n",
        "\n",
        "# also, trained word vectors are stored in a KeyedVectors instance in model.wv:\n",
        "\n",
        "len(w2v_model.wv.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8KfExiixhNw"
      },
      "source": [
        "#we do not plan to train the model any further, we are calling init_sims(), which will make the model much more memory-efficient:\n",
        "w2v_model.init_sims(replace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E_Y-RYhxn0o"
      },
      "source": [
        "word = 'covid'\n",
        "if word in w2v_model.wv.vocab:\n",
        "    print(word)\n",
        "\n",
        "# getting count\n",
        "vocab_obj =  w2v_model.wv.vocab[\"machine\"]\n",
        "vocab_obj.count\n",
        "\n",
        "w2c = dict()\n",
        "for item in w2v_model.wv.vocab:\n",
        "    w2c[item]=w2v_model.wv.vocab[item].count\n",
        "print(w2c)\n",
        "# sorted on freq\n",
        "w2cSorted=dict(sorted(w2c.items(), key=lambda x: x[1],reverse=True))\n",
        "print(w2cSorted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCGanLmExr38"
      },
      "source": [
        "w2v_model.wv.most_similar(positive=[\"covid\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwwIcmqExuYh"
      },
      "source": [
        "w2v_model.wv.most_similar(positive=[\"robot\"], topn = 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1hw-HjRxyYG"
      },
      "source": [
        "## Playing with model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h06fn8HBx1bT"
      },
      "source": [
        "# TRY this\n",
        "word_vectors = pd.DataFrame(w2v_model.wv.vectors, index=word2vec.wv.index2word)\n",
        "print('word vector dimension: ', word_vectors.shape)\n",
        "word_vectors.head()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LySokopIx4tT"
      },
      "source": [
        "# list of all words known to the model in model.wv.index2word\n",
        "import random\n",
        "#get the key, w2v_model.wv.index2word\n",
        "print(random.choice(model.wv.index2entity) \n",
        "#get vector, w2v_model.wv[w2v_model.wv.index2word]\n",
        "print(model.wv[random.choice(model.wv.index2entity])\n",
        "\n",
        " \n",
        "# Get a list of words in the vocabulary\n",
        "words = model.wv.vocab.keys()\n",
        "# Make a dictionary\n",
        "we_dict = {word:model.wv[word] for word in words}\n",
        "\n",
        "# get vectors, model[word]\n",
        "model.wv.vectors\n",
        "# list of words in the right order, with sync0\n",
        "model.index2word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7uJ5beDx6zd"
      },
      "source": [
        "# look for words\n",
        "if 'country' in my_model:\n",
        "    print(my_model['country'][0:10])\n",
        "else: \n",
        "    pass \n",
        "\n",
        "#or\n",
        "try:\n",
        "    print(my_model['country'][0:10])\n",
        "except:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yiOyH2hx8pQ"
      },
      "source": [
        "w2v_model.wv.most_similar(positive=[\"beer\"])\n",
        "w2v_model.wv.doesnt_match(\"apple microsoft samsung tesla\".split())\n",
        "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBj4AzkVyO7h"
      },
      "source": [
        "# Keyword Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78ejoIZ3yR-h"
      },
      "source": [
        "import re\n",
        "import numpy\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from numpy import *\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "with open(\"/content/drive/My Drive/RokinData/word2vec/b.txt\") as file:\n",
        "    text_review = file.read()\n",
        "\n",
        "#if you want to use Google original vectors from Google News corpora\n",
        "# model = word2vec.Word2Vec.load_word2vec_format('/Users/Downloads/GoogleNews-vectors-negative300.bin', binary=True)\n",
        "#if you want to use your own vector\n",
        "model = Word2Vec.load(\"/content/drive/My Drive/RokinData/models/word2vec_50k.model\")\n",
        "\n",
        "def text_to_wordlist(text, remove_stopwords=True):\n",
        "    # 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "\n",
        "    # 3. Convert words to lower case and split them, clean stopwords from model' vocabulary\n",
        "    words = review_text.lower().split()\n",
        "    stops = set(stopwords.words('english'))\n",
        "    meaningful_words = [w for w in words if not w in stops]\n",
        "    return (meaningful_words)\n",
        "\n",
        "\n",
        "# Function to get feature vec of words\n",
        "def get_feature_vec(words, model):\n",
        "    # Index2word is a list that contains the names of the words in\n",
        "    # the model's vocabulary. Convert it to a set, for speed \n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    clean_text = []\n",
        "    # vocabulary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            clean_text.append(model[word])\n",
        "\n",
        "    return clean_text\n",
        "\n",
        "\n",
        "# bag of word list without stopwords\n",
        "clean_train_text = (text_to_wordlist(text_review, remove_stopwords=True))\n",
        "\n",
        "# delete words which occur more than ones\n",
        "clean_train = []\n",
        "for words in clean_train_text:\n",
        "    if words in clean_train:\n",
        "        words = +1\n",
        "    else:\n",
        "        clean_train.append(words)\n",
        "\n",
        "trainDataVecs = get_feature_vec(clean_train, model)\n",
        "trainData = numpy.asarray(trainDataVecs)\n",
        "\n",
        "# calculate cosine similarity matrix to use in pagerank algorithm for dense matrix, it is not\n",
        "# fast for sparse matrix\n",
        "# sim_matrix = 1-pairwise_distances(trainData, metric=\"cosine\")\n",
        "\n",
        "# similarity matrix, it is 30 times faster for sparse matrix\n",
        "# replace this with A.dot(A.T).todense() for sparse representation\n",
        "similarity = numpy.dot(trainData, trainData.T)\n",
        "\n",
        "# squared magnitude of preference vectors (number of occurrences)\n",
        "square_mag = numpy.diag(similarity)\n",
        "\n",
        "# inverse squared magnitude\n",
        "inv_square_mag = 1 / square_mag\n",
        "\n",
        "# if it doesn't occur, set it's inverse magnitude to zero (instead of inf)\n",
        "inv_square_mag[numpy.isinf(inv_square_mag)] = 0\n",
        "\n",
        "# inverse of the magnitude\n",
        "inv_mag = numpy.sqrt(inv_square_mag)\n",
        "\n",
        "# cosine similarity (elementwise multiply by inverse magnitudes)\n",
        "cosine = similarity * inv_mag\n",
        "cosine = cosine.T * inv_mag\n",
        "\n",
        "\n",
        "# pagerank powermethod\n",
        "def powerMethod(A, x0, m, iter):\n",
        "    n = A.shape[1]\n",
        "    delta = m * (array([1] * n, dtype='float64') / n)\n",
        "    for i in range(iter):\n",
        "        x0 = dot((1 - m), dot(A, x0)) + delta\n",
        "    return x0\n",
        "\n",
        "\n",
        "n = cosine.shape[1]  # A is n x n\n",
        "m = 0.15\n",
        "x0 = [1] * n\n",
        "\n",
        "pagerank_values = powerMethod(cosine, x0, m, 130)\n",
        "\n",
        "srt = numpy.argsort(-pagerank_values)\n",
        "a = srt[0:10]\n",
        "\n",
        "keywords_list = []\n",
        "\n",
        "for words in a:\n",
        "    keywords_list.append(clean_train_text[words])\n",
        "    \n",
        "print(keywords_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2jNY6UMyZpQ"
      },
      "source": [
        "## Restricting to custom vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "325YF3JBydBR"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def restrict_w2v(w2v, restricted_word_set):\n",
        "    new_vectors = []\n",
        "    new_vocab = {}\n",
        "    new_index2entity = []\n",
        "    new_vectors_norm = []\n",
        "\n",
        "    for i in range(len(w2v.vocab)):\n",
        "        word = w2v.index2entity[i]\n",
        "        vec = w2v.vectors[i]\n",
        "        vocab = w2v.vocab[word]\n",
        "        vec_norm = w2v.vectors_norm[i]\n",
        "        if word in restricted_word_set:\n",
        "            vocab.index = len(new_index2entity)\n",
        "            new_index2entity.append(word)\n",
        "            new_vocab[word] = vocab\n",
        "            new_vectors.append(vec)\n",
        "            new_vectors_norm.append(vec_norm)\n",
        "\n",
        "    w2v.vocab = new_vocab\n",
        "    w2v.vectors = np.array(new_vectors)\n",
        "    w2v.index2entity = np.array(new_index2entity)\n",
        "    w2v.index2word = np.array(new_index2entity)\n",
        "    w2v.vectors_norm = np.array(new_vectors_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxIQDor4yf1-"
      },
      "source": [
        "words = ['microsoft', 'bing', 'windows','google', 'python', 'scala', 'siemens', 'erlangen','germany','france','spain','tesla','thomas','john','chun','chan','china',\n",
        "         'internet_security','internet','intel','amd','qualcomm','nvidia','cable','modem','chip_set'\n",
        "         'america','us','north_america','canada','europe','germany','dortmund','munich','france','italy','aachen','austria','regensburg','bavaria','bremen',\n",
        "         'electric_car','energy_storage','nissan_leaf','tesla','roadster',\n",
        "         'stanford_university','university_washington','university_toronto','mit',\n",
        "         'coronavirus','covid','pandemic','ebola',\n",
        "         'quantum technology','astrophysics','qubit','quantum computer','electron', 'quantum',\n",
        "         'industrial_ethernet', 'ethernet_protocol','kirigami','origami','sculpture','architecture','japanese',\n",
        "         'robotaxi','san_francisco', 'general_motor', 'magnetic_sensor', 'flexible_electronics','organic_electronics','electronic_skin','elastic_surface','active_matrix','artificial_skin',\n",
        "         'robot','artificial_intelligence', 'workplace_robot','digital_transformation','iot','connected_device','smart_home','industrial_iot','cloud_computing','vitualization','severless',\n",
        "         'starbucks','cofee','carbon_nanotube','aerospace','aerospace_manufacturing','composite_manufacturing','fuselage', 'aircraft','wing','airplane',\n",
        "         'honda','fuel_cell','zero_emission','hydrogen_fuel','duty_truck','sustainable_energy',\n",
        "         'phishing_attack','ransomware','inbox_infiltration','phishing_email','malicious','scam','login_page','attack','fraud']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P10pPzbSyiaF"
      },
      "source": [
        "restricted_word_set = {\"beer\", \"wine\", \"computer\", \"python\", \"bash\", \"lagers\"}\n",
        "# restricted_word_set = set(words)\n",
        "\n",
        "new = []\n",
        "for w in restricted_word_set:\n",
        "  if w in w2v_model.wv.vocab:\n",
        "    new.append(w)\n",
        "\n",
        "restrict_w2v(w2v_model.wv, new)\n",
        "w2v_model.wv.most_similar(positive=[\"cofee\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3h9IcoSyk-O"
      },
      "source": [
        "# restricted_word_set = {\"beer\", \"wine\", \"computer\", \"python\", \"bash\", \"lagers\"}\n",
        "\n",
        "restrict_w2v(w2v_model.wv, restricted_word_set)\n",
        "w2v_model.wv.most_similar(positive=[\"cofee\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIZDeDJBypeD"
      },
      "source": [
        "## Extracting vector for given word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK1b0UkJys85"
      },
      "source": [
        "# extracting vectors for only given vectors\n",
        "words = ['microsoft', 'bing', 'windows','google', 'python', 'scala', 'siemens', 'erlangen','germany','france','spain','tesla','thomas','john','chun','chan','china',\n",
        "         'internet_security','internet','intel','amd','qualcomm','nvidia','cable','modem','chip_set'\n",
        "         'america','us','north_america','canada','europe','germany','dortmund','munich','france','italy','aachen','austria','regensburg','bavaria','bremen',\n",
        "         'electric_car','energy_storage','nissan_leaf','tesla','roadster',\n",
        "         'stanford_university','university_washington','university_toronto','mit',\n",
        "         'coronavirus','covid','pandemic','ebola',\n",
        "         'quantum technology','astrophysics','qubit','quantum computer','electron', 'quantum',\n",
        "         'industrial_ethernet', 'ethernet_protocol','kirigami','origami','sculpture','architecture','japanese',\n",
        "         'robotaxi','san_francisco', 'general_motor', 'magnetic_sensor', 'flexible_electronics','organic_electronics','electronic_skin','elastic_surface','active_matrix','artificial_skin',\n",
        "         'robot','artificial_intelligence', 'workplace_robot','digital_transformation','iot','connected_device','smart_home','industrial_iot','cloud_computing','vitualization','severless',\n",
        "         'starbucks','cofee','carbon_nanotube','aerospace','aerospace_manufacturing','composite_manufacturing','fuselage', 'aircraft','wing','airplane',\n",
        "         'honda','fuel_cell','zero_emission','hydrogen_fuel','duty_truck','sustainable_energy',\n",
        "         'phishing_attack','ransomware','inbox_infiltration','phishing_email','malicious','scam','login_page','attack','fraud']\n",
        "words[:2]\n",
        "\n",
        "# load model\n",
        "\n",
        "## Check dimension of word vectors\n",
        "w2v_model.vector_size\n",
        "\n",
        "# pass words thr this to get vector\n",
        "# also make sure its in vocab of model: if word in model.vocab\n",
        "\n",
        "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
        "vector_list = [w2v_model[word] for word in words if word in w2v_model.wv.vocab]\n",
        "\n",
        "# Create a list of the words corresponding to these vectors\n",
        "words_filtered = [word for word in words if word in w2v_model.wv.vocab]\n",
        "\n",
        "# Zip the words together with their vector representations\n",
        "word_vec_zip = zip(words_filtered, vector_list)\n",
        "\n",
        "# Cast to a dict so we can turn it into a DataFrame\n",
        "word_vec_dict = dict(word_vec_zip)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
        "\n",
        "print(df.info())\n",
        "df.head(3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2R48LXEyyVf"
      },
      "source": [
        "words = df.index\n",
        "words\n",
        "\n",
        "# df1 = df.rename_axis(None)\n",
        "df1 = df.reset_index()\n",
        "df2 = df1.iloc[:, 0:300]\n",
        "df2.drop(columns =['index'],inplace=True)\n",
        "\n",
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMGNcl4Ly7AW"
      },
      "source": [
        "# Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Voet4MBwy9a_"
      },
      "source": [
        "# Word Vectors for each word in the vocab,\n",
        "Z = w2v_model.wv.syn0;\n",
        "print(Z[0].shape)\n",
        "Z[0]\n",
        "# model.syn1[model.vocab[word].index]\n",
        "\n",
        "## Setting word and corresponding vector together \n",
        "\n",
        "#zip the two lists containing vectors and words\n",
        "zipped = zip(nmodel.wv.index2word, nmodel.wv.syn0)\n",
        "\n",
        "#the resulting list contains `(word, wordvector)` tuples. We can extract the entry for any `word` or `vector` (replace with the word/vector you're looking for) using a list comprehension:\n",
        "wordresult = [i for i in zipped if i[0] == word]\n",
        "vecresult = [i for i in zipped if i[1] == vector]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-io1TSnUzAw5"
      },
      "source": [
        "# kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
        "    # Initalize a k-means object and use it to extract centroids\n",
        "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n",
        "    idx = kmeans_clustering.fit_predict(word_vectors);\n",
        "    \n",
        "    return kmeans_clustering.cluster_centers_, idx;\n",
        "\n",
        "# 50 clusters\n",
        "centers, clusters = clustering_on_wordvecs(Z, 50);\n",
        "centroid_map = dict(zip(w2v_model.wv.index2word, clusters));\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqJvxy_-zDHV"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import KDTree\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import cycle\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "\n",
        "#get words in each cluster that are closest to the cluster center.\n",
        "def get_top_words(index2word, k, centers, wordvecs):\n",
        "    tree = KDTree(wordvecs);\n",
        "#Closest points for each Cluster center is used to query the closest 20 points to it.\n",
        "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n",
        "    closest_words_idxs = [x[1] for x in closest_points];\n",
        "#Word Index is queried for each position in the above array, and added to a Dictionary.\n",
        "    closest_words = {};\n",
        "    for i in range(0, len(closest_words_idxs)):\n",
        "        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
        "#A DataFrame is generated from the dictionary.\n",
        "    df = pd.DataFrame(closest_words);\n",
        "    df.index = df.index+1\n",
        "    return df;\n",
        "\n",
        "top_words = get_top_words(w2v_model.wv.index2word, 5000, centers, Z);\n",
        "\n",
        "# Word cloud visualization\n",
        "def display_cloud(cluster_num, cmap):\n",
        "    wc = WordCloud(background_color=\"black\", max_words=2000, max_font_size=80, colormap=cmap);\n",
        "    wordcloud = wc.generate(' '.join([word for word in top_words['Cluster #' + str(cluster_num)]]))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.savefig('cluster_' + str(cluster_num), bbox_inches='tight')\n",
        "\n",
        "cmaps = cycle([\n",
        "            'flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',\n",
        "            'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg', 'hsv',\n",
        "            'gist_rainbow', 'rainbow', 'jet', 'nipy_spectral', 'gist_ncar'])\n",
        "for i in range(50):\n",
        "    col = next(cmaps);\n",
        "    display_cloud(i, col)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbf2enSWzFUW"
      },
      "source": [
        "# deleting the png files\n",
        "!rm /content/cluster_{0..49}.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFvFDw41zHO9"
      },
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "# pd.set_option('display.max_rows', None)\n",
        "\n",
        "from google.colab import data_table\n",
        "data_table.DataTable(top_words)\n",
        "# top_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNLPC28RzL0m"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTjsWE_XzV2u"
      },
      "source": [
        "## Tsne"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ghc7JaRzO9q"
      },
      "source": [
        "# t-sne visualization\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APCd8EhFzQwM"
      },
      "source": [
        "def tsne_plot(model):\n",
        "    \"Creates and TSNE model and plots it\"\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    for word in model.wv.vocab:\n",
        "        tokens.append(model[word])\n",
        "        labels.append(word)\n",
        "    \n",
        "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
        "    new_values = tsne_model.fit_transform(tokens)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.figure(figsize=(16, 16)) \n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "    plt.show()\n",
        "\n",
        "tsne_plot(w2v_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ujzkisezXv7"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NvRfbjyzZZy"
      },
      "source": [
        "# /content/drive/My Drive/RokinData/models/word2vec.model\n",
        "w2v_model.wv.save_word2vec_format('/content/logs/myW2V')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSSnhLhiztwK"
      },
      "source": [
        "!python -m gensim.scripts.word2vec2tensor -i /content/logs/myW2V -o /content/logs/Mymodel "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3KXmTxNzv5X"
      },
      "source": [
        "import io\n",
        "# ONE LINER\n",
        "# !python -m gensim.scripts.word2vec2tensor -i ~/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz \n",
        "\n",
        "w2v = Word2Vec.load(\"/content/drive/My Drive/RokinData/models/word2vec_50k.model\")\n",
        "# Vector file, `\\t` seperated the vectors and `\\n` seperate the words\n",
        "\"\"\"\n",
        "0.1\\t0.2\\t0.5\\t0.9\n",
        "0.2\\t0.1\\t5.0\\t0.2\n",
        "0.4\\t0.1\\t7.0\\t0.8\n",
        "\"\"\"\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "# Meta data file, `\\n` seperated word\n",
        "\"\"\"\n",
        "token1\n",
        "token2\n",
        "token3\n",
        "\"\"\"\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "# Write meta file and vector file\n",
        "for index in range(len(w2v.wv.index2word)):\n",
        "    word = w2v.wv.index2word[index]\n",
        "    vec = w2v.wv.vectors[index]\n",
        "    out_m.write(word + \"\\n\")\n",
        "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()\n",
        "\n",
        "\n",
        "# Then we can visuale using the `http://projector.tensorflow.org/` to visualize those two files.\n",
        "\n",
        "# 1. Open the Embedding Projector.\n",
        "# 2. Click on \"Load data\".\n",
        "# 3. Upload the two files we created above: vecs.tsv and meta.tsv."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayw9Fa9nz4jG"
      },
      "source": [
        "### Experiment: visualize in colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6g712ILz8K5"
      },
      "source": [
        "# test 1\n",
        "# youtube similar to abv, remaining below\n",
        "# model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/RokinData/models/word2vec.model')\n",
        "import os\n",
        "\n",
        "tsv_file_path = \"/content/tensorboard/metadata.tsv\"\n",
        "path = '/content/tensorboard'\n",
        "\n",
        "model = gensim.models.keyedvectors.KeyedVectors.load('/content/drive/My Drive/RokinData/models/word2vec.model')\n",
        "max_size = len(model.wv.vocab)-1\n",
        "w2v = np.zeros((max_size,model.layer1_size))\n",
        "\n",
        "if not os.path.exists('tensorboard'):\n",
        " os.makedirs('tensorboard')\n",
        "\n",
        "with open(tsv_file_path,'w+') as file_metadata:\n",
        "    for i,word in enumerate(model.wv.index2word[:max_size]):\n",
        "        w2v[i] = model.wv[word]\n",
        "        file_metadata.write(word+'\\n')\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "with tf.device(\"/cpu:0\"):\n",
        "  embedding = tf.Variable(w2v, trainable=False, name='embedding')\n",
        "\n",
        "tf.global_variables_initializer().run()\n",
        "\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "writer= tf.summary.FileWriter(path, sess.graph)\n",
        "\n",
        "config=projector.ProjectorConfig()\n",
        "embed=config.embedding.add()\n",
        "embed.tensor_name = 'embeddings'\n",
        "embed.metadata_path = 'metadata.tsv' #/content/tensorboard/metadata.tsv\n",
        "\n",
        "\n",
        "projector.visualize_embeddings(writer,config)\n",
        "saver.save(sess,path + '/model.ckpt', global_step=max_size)\n",
        "\n",
        "# now take terminal tensorboard --logdir= '..../tensorboard' --port =8080\n",
        "# http://localhost:8080/\n",
        "\n",
        "# go to tensorboard projector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKVvhQ4f0BqV"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# %load_ext tensorboard.notebook\n",
        "\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec,KeyedVectors\n",
        "\n",
        "\n",
        "# load the model\n",
        "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/RokinData/models/GoogleNews-vectors-negative300-SLIM.bin', binary=True)\n",
        "\n",
        "print(\"Vocabulary Size: {0}\".format(len(model.vocab)))\n",
        "model[\"for\"].shape\n",
        "\n",
        "\n",
        "# numpy array to store vocab\n",
        "import numpy as np\n",
        "#Important Parameters\n",
        "VOCAB_SIZE = len(model.vocab)\n",
        "EMBEDDING_DIM = model[\"is\"].shape[0]\n",
        "w2v = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
        "\n",
        "tsv_file_path = \"/content/tensorboard/metadata.tsv\"\n",
        "with open(tsv_file_path,'w+', encoding='utf-8') as file_metadata:\n",
        "    for i,word in enumerate(model.index2word[:VOCAB_SIZE]):\n",
        "        w2v[i] = model[word]\n",
        "        file_metadata.write(word+'\\n')\n",
        "\n",
        "import tensorflow as tf\n",
        "# from tensorflow.contrib.tensorboard.plugins import projector\n",
        "from tensorboard.plugins import projector\n",
        "TENSORBOARD_FILES_PATH = \"/content/tensorboard/tensorboard\"\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "#Tensorflow Placeholders\n",
        "X_init = tf.placeholder(tf.float32, shape=(VOCAB_SIZE, EMBEDDING_DIM), name=\"embedding\")\n",
        "X = tf.Variable(X_init)\n",
        "#Initializer\n",
        "init = tf.global_variables_initializer()\n",
        "#Start Tensorflow Session\n",
        "sess = tf.Session()\n",
        "sess.run(init, feed_dict={X_init: w2v})\n",
        "#Instance of Saver, save the graph.\n",
        "saver = tf.train.Saver()\n",
        "writer = tf.summary.FileWriter(TENSORBOARD_FILES_PATH, sess.graph)\n",
        "\n",
        "\n",
        "#Configure a Tensorflow Projector\n",
        "config = projector.ProjectorConfig()\n",
        "embed = config.embeddings.add()\n",
        "embed.metadata_path = tsv_file_path\n",
        "#Write a projector_config\n",
        "projector.visualize_embeddings(writer,config)\n",
        "#save a checkpoint\n",
        "saver.save(sess, TENSORBOARD_FILES_PATH+'/model.ckpt', global_step = VOCAB_SIZE)\n",
        "#close the session\n",
        "sess.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoEUpjbS0EJh"
      },
      "source": [
        "!python -m tensorboard.main --logdir='/content/tensorboard'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbT53GVY0H0d"
      },
      "source": [
        "!pip install tb-nightly\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "vectors = np.array([[0,0,1], [0,1,0], [1,0,0], [1,1,1]])\n",
        "metadata = ['001', '010', '100', '111']   # labels\n",
        "\n",
        "writer = SummaryWriter()\n",
        "writer.add_embedding(vectors, metadata)\n",
        "writer.close()\n",
        "\n",
        "# !kill 444\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg9rBGYP0OsT"
      },
      "source": [
        "# Pre trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pze9hTH0Qsd"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# !gunzip '/content/drive/My Drive/RokinData/models/GoogleNews-vectors-negative300-SLIM.bin.gz'\n",
        "# Load Google's pre-trained Word2Vec model.\n",
        "\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/RokinData/models/GoogleNews-vectors-negative300-SLIM.bin', binary=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjJipquc0UHS"
      },
      "source": [
        "print(model.most_similar(\"linux\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML10ZpRa0WW2"
      },
      "source": [
        "# /content/drive/My Drive/RokinData/models/glove.6B.300d.txt\n",
        "\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove2word2vec(glove_input_file=\"/content/drive/My Drive/RokinData/models/glove.6B.300d.txt\", word2vec_output_file=\"/content/drive/My Drive/RokinData/models/gensim_glove_vectors.txt\")\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "glove_model = KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/RokinData/models/gensim_glove_vectors.txt\", binary=False)\n",
        "\n",
        "print(glove_model.most_similar(\"apple\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfRO7S4s0Z6E"
      },
      "source": [
        "## add new data to pre trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmXnubzh0dSn"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/RokinData/models/gensim_glove_vectors.txt\", binary=False)\n",
        "\n",
        "# train actually but here we use pre saved\n",
        "my_model = Word2Vec.load(\"/content/drive/My Drive/RokinData/models/word2vec.model\")\n",
        "\n",
        "# ------> if training\n",
        "# my_model = Word2Vec(size=300, min_count=1)\n",
        "# my_model.build_vocab(sentences)\n",
        "# total_examples = my_model.corpus_count\n",
        "\n",
        "total_examples = my_model.corpus_count\n",
        "my_model.build_vocab([list(model.vocab.keys())], update=True)\n",
        "my_model.intersect_word2vec_format(\"/content/drive/My Drive/RokinData/models/gensim_glove_vectors.txt\", binary=False, lockf=1.0)\n",
        "\n",
        "my_model.train(sentences, total_examples=total_examples, epochs=my_model.iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCynP9fV0hoN"
      },
      "source": [
        "my_model.wv.most_similar(positive=[\"microsoft\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}