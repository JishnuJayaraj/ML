{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS tagging.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMcgBpJJkSzz1eeNjVkWPNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JishnuJayaraj/ML/blob/master/NLP/Bert/POS%20tagging/POS_tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9NrQ9Hm5DDz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh5vON6lWzA"
      },
      "source": [
        "#BERT POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "903vAzXpuT6S"
      },
      "source": [
        "#GET TF1 beahviour in 2\n",
        "#import tensorflow.compat.v1 as tf\n",
        "#tf.disable_v2_behavior()\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "! pip install bert-tensorflow\n",
        "! pip install pyconll\n",
        "\n",
        "import pyconll, keras, pickle, os, random, nltk, datetime, warnings, gc, urllib.request, zipfile, collections\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import load_model\n",
        "from keras.utils import plot_model\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "from keras.layers import Embedding, Dense, Input, concatenate, Layer, Lambda, Dropout, Activation\n",
        "from keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from bert.tokenization import FullTokenizer\n",
        "\n",
        "from tqdm import tqdm_notebook\n",
        "from IPython.display import Image \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ9aLQM0IAef",
        "outputId": "94c5ac49-1f08-46c9-dfe5-8c2084a8123a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# train data\n",
        "UD_ENGLISH_TRAIN = 'en_partut-ud-train.conllu'\n",
        "UD_ENGLISH_DEV = 'en_partut-ud-dev.conllu'\n",
        "UD_ENGLISH_TEST = 'en_partut-ud-test.conllu'\n",
        "\n",
        "def download_files():\n",
        "    print('Downloading English treebank...')\n",
        "    urllib.request.urlretrieve('http://archive.aueb.gr:8085/files/en_partut-ud-dev.conllu', 'en_partut-ud-dev.conllu')\n",
        "    urllib.request.urlretrieve('http://archive.aueb.gr:8085/files/en_partut-ud-test.conllu', 'en_partut-ud-test.conllu')\n",
        "    urllib.request.urlretrieve('http://archive.aueb.gr:8085/files/en_partut-ud-train.conllu', 'en_partut-ud-train.conllu')\n",
        "    print('Treebank downloaded.')\n",
        "\n",
        "download_files()\n",
        "\n",
        "def read_conllu(path):\n",
        "    data = pyconll.load_from_file(path)\n",
        "    tagged_sentences=[]\n",
        "    t=0\n",
        "    for sentence in data:\n",
        "        tagged_sentence=[]\n",
        "        for token in sentence:\n",
        "            if token.upos and token.form:\n",
        "                t+=1\n",
        "                tagged_sentence.append((token.form.lower(), token.upos))\n",
        "        tagged_sentences.append(tagged_sentence)\n",
        "    return tagged_sentences\n",
        "\n",
        "train_sentences = read_conllu(UD_ENGLISH_TRAIN)\n",
        "val_sentences = read_conllu(UD_ENGLISH_DEV)\n",
        "test_sentences = read_conllu(UD_ENGLISH_TEST)\n",
        "\n",
        "# Some usefull functions\n",
        "def tag_sequence(sentences):\n",
        "    return [[t for w, t in sentence] for sentence in sentences]\n",
        "\n",
        "def text_sequence(sentences):\n",
        "    return [[w for w, t in sentence] for sentence in sentences]\n",
        "\n",
        "tags = set([item for sublist in train_sentences+test_sentences+val_sentences for _, item in sublist])\n",
        "\n",
        "tag2int = {}\n",
        "int2tag = {}\n",
        "\n",
        "for i, tag in enumerate(sorted(tags)):\n",
        "    tag2int[tag] = i+1\n",
        "    int2tag[i+1] = tag\n",
        "\n",
        "# Special character for the tags\n",
        "tag2int['-PAD-'] = 0\n",
        "int2tag[0] = '-PAD-'\n",
        "\n",
        "n_tags = len(tag2int)\n",
        "print('Total tags:', n_tags)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading English treebank...\n",
            "Treebank downloaded.\n",
            "Total tags: 18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFRhsgMWYtpS"
      },
      "source": [
        "# Initialize session\n",
        "sess = tf.compat.v1.Session()\n",
        "# Params for bert model and tokenization\n",
        "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "1165aa8f-9bd2-48d7-8ad2-3109c960334f"
        },
        "id": "cTp5QQi0l9rE"
      },
      "source": [
        "class PaddingInputExample(object):\n",
        "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\"\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "    \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "def create_tokenizer_from_hub_module():\n",
        "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
        "    bert_module =  hub.Module(bert_path)\n",
        "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
        "    vocab_file, do_lower_case = sess.run(\n",
        "        [\n",
        "            tokenization_info[\"vocab_file\"],\n",
        "            tokenization_info[\"do_lower_case\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
        "\n",
        "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
        "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "    if isinstance(example, PaddingInputExample):\n",
        "        input_ids = [0] * max_seq_length\n",
        "        input_mask = [0] * max_seq_length\n",
        "        segment_ids = [0] * max_seq_length\n",
        "        label_ids = [0] * max_seq_length\n",
        "        return input_ids, input_mask, segment_ids, label_ids\n",
        "    \n",
        "    tokens_a = example.text_a\n",
        "    if len(tokens_a) > max_seq_length-2:\n",
        "        tokens_a = tokens_a[0 : (max_seq_length-2)]\n",
        "\n",
        "# Token map will be an int -> int mapping between the `orig_tokens` index and\n",
        "# the `bert_tokens` index.\n",
        "\n",
        "# bert_tokens == [\"[CLS]\", \"john\", \"johan\", \"##son\", \"'\", \"s\", \"house\", \"[SEP]\"]\n",
        "# orig_to_tok_map == [1, 2, 4, 6]   \n",
        "    orig_to_tok_map = []              \n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    \n",
        "    tokens.append(\"[CLS]\")\n",
        "    segment_ids.append(0)\n",
        "    orig_to_tok_map.append(len(tokens)-1)\n",
        "    #print(len(tokens_a))\n",
        "    for token in tokens_a:       \n",
        "        tokens.extend(tokenizer.tokenize(token))\n",
        "        orig_to_tok_map.append(len(tokens)-1)\n",
        "        segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "    orig_to_tok_map.append(len(tokens)-1)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids([tokens[i] for i in orig_to_tok_map])\n",
        "    #print(len(orig_to_tok_map), len(tokens), len(input_ids), len(segment_ids)) #for debugging\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "    \n",
        "    label_ids = []\n",
        "    labels = example.label\n",
        "    label_ids.append(0)\n",
        "    label_ids.extend([tag2int[label] for label in labels])\n",
        "    label_ids.append(0)\n",
        "    #print(len(label_ids)) #for debugging\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < max_seq_length:\n",
        "        input_ids.append(0)\n",
        "        input_mask.append(0)\n",
        "        segment_ids.append(0)\n",
        "        label_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "    assert len(label_ids) == max_seq_length\n",
        "\n",
        "    return input_ids, input_mask, segment_ids, label_ids\n",
        "\n",
        "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
        "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
        "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
        "        input_id, input_mask, segment_id, label = convert_single_example(\n",
        "            tokenizer, example, max_seq_length\n",
        "        )\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        labels.append(label)\n",
        "    return (\n",
        "        np.array(input_ids),\n",
        "        np.array(input_masks),\n",
        "        np.array(segment_ids),\n",
        "        np.array(labels),\n",
        "    )\n",
        "\n",
        "def convert_text_to_examples(texts, labels):\n",
        "    \"\"\"Create InputExamples\"\"\"\n",
        "    InputExamples = []\n",
        "    for text, label in zip(texts, labels):\n",
        "        InputExamples.append(\n",
        "            InputExample(guid=None, text_a=text, text_b=None, label=label)\n",
        "        )\n",
        "    return InputExamples\n",
        "\n",
        "\n",
        "\n",
        "# Build model\n",
        "def build_model(max_seq_length):\n",
        "    seed = 0 \n",
        "    in_id = keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "    in_mask = keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "    in_segment = keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "    bert_inputs = [in_id, in_mask, in_segment]\n",
        "    \n",
        "    np.random.seed(seed)\n",
        "    bert_output = BertLayer()(bert_inputs)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    outputs = keras.layers.Dense(n_tags, activation=keras.activations.softmax)(bert_output)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    model = keras.models.Model(inputs=bert_inputs, outputs=outputs)\n",
        "    np.random.seed(seed)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr=0.00004), loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])   \n",
        "    model.summary(100)\n",
        "    return model\n",
        "\n",
        "def initialize_vars(sess):\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.tables_initializer())\n",
        "    K.set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbTDTysjZ5V4",
        "outputId": "3d04e23f-6337-4167-9ade-3764039dd8e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Instantiate tokenizer\n",
        "tokenizer = create_tokenizer_from_hub_module()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSQfUeC8aFcS"
      },
      "source": [
        "def bert_labels(labels):\n",
        "    train_label_bert = []\n",
        "    train_label_bert.append('-PAD-')\n",
        "    for i in labels:\n",
        "        train_label_bert.append(i)\n",
        "    train_label_bert.append('-PAD-')\n",
        "    print('BERT labels:', train_label_bert) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LatHKTfhx5dy"
      },
      "source": [
        "class BertLayer(Layer):\n",
        "    def __init__(self, output_representation='sequence_output', trainable=True, **kwargs):\n",
        "        self.bert = None\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "        \n",
        "        self.trainable = trainable\n",
        "        self.output_representation = output_representation\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # SetUp tensorflow Hub module\n",
        "        self.bert = hub.Module(bert_path,\n",
        "                               trainable=self.trainable, \n",
        "                               name=\"{}_module\".format(self.name))\n",
        "\n",
        "        # Assign module's trainable weights to model\n",
        "        # Remove unused layers and set trainable parameters\n",
        "        # s = [\"/cls/\", \"/pooler/\", 'layer_11', 'layer_10', 'layer_9', 'layer_8', 'layer_7', 'layer_6']\n",
        "        s = [\"/cls/\", \"/pooler/\"]\n",
        "        self.trainable_weights += [var for var in self.bert.variables[:] if not any(x in var.name for x in s)]\n",
        "            \n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "                \n",
        "        # See Trainable Variables\n",
        "        #tf.logging.info(\"**** Trainable Variables ****\")\n",
        "        #for var in self.trainable_weights:\n",
        "        #    init_string = \", *INIT_FROM_CKPT*\"\n",
        "        #    tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape, init_string)\n",
        "            \n",
        "        print('Trainable weights:',len(self.trainable_weights))\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "            self.output_representation\n",
        "        ]\n",
        "        return result\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return K.not_equal(inputs[0], 0.0)   \n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        if self.output_representation == 'pooled_output':\n",
        "            return (None, 768)\n",
        "        else:\n",
        "            return (None, None, 768)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEFyZGTEULyd"
      },
      "source": [
        "## Loading model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFstGbchuYaE",
        "outputId": "f9013c42-beac-4ebb-805c-b219c1605e6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "\n",
        "MAX_SEQUENCE_LENGTH = 70\n",
        "initialize_vars(sess)\n",
        "\n",
        "model = build_model(MAX_SEQUENCE_LENGTH+2) \n",
        "model.load_weights('POS.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trainable weights: 197\n",
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "____________________________________________________________________________________________________\n",
            "Layer (type)                     Output Shape          Param #     Connected to                     \n",
            "====================================================================================================\n",
            "input_ids (InputLayer)           (None, 72)            0                                            \n",
            "____________________________________________________________________________________________________\n",
            "input_masks (InputLayer)         (None, 72)            0                                            \n",
            "____________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)         (None, 72)            0                                            \n",
            "____________________________________________________________________________________________________\n",
            "bert_layer_2 (BertLayer)         (None, None, 768)     110104890   input_ids[0][0]                  \n",
            "                                                                   input_masks[0][0]                \n",
            "                                                                   segment_ids[0][0]                \n",
            "____________________________________________________________________________________________________\n",
            "dense_1 (Dense)                  (None, None, 18)      13842       bert_layer_2[0][0]               \n",
            "====================================================================================================\n",
            "Total params: 110,118,732\n",
            "Trainable params: 108,905,490\n",
            "Non-trainable params: 1,213,242\n",
            "____________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HijNdlroJWQK"
      },
      "source": [
        "## Give the new data here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgBE4POrudmU",
        "outputId": "dd79a5fa-42fd-4d89-ae7e-f131ae8b4d11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sentence_raw = 'PLM: Poorly Digitized Labs Give Dassault Systèmes’ BIOVIA a Great Growth Potential'\n",
        "\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "sentence_ini = word_tokenize(sentence_raw.lower())\n",
        "\n",
        "sentence_bert = tokenizer.tokenize(sentence_raw)\n",
        "\n",
        "tokens_a = sentence_ini"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTEyfb8Uu5he"
      },
      "source": [
        "orig_to_tok_map = []              \n",
        "tokens = []\n",
        "segment_ids = []\n",
        "tokens.append(\"[CLS]\")\n",
        "segment_ids.append(0)\n",
        "orig_to_tok_map.append(len(tokens)-1)\n",
        "for token in tokens_a:\n",
        "    #orig_to_tok_map.append(len(tokens)) # keep first piece of tokenized term\n",
        "    tokens.extend(tokenizer.tokenize(token))\n",
        "    orig_to_tok_map.append(len(tokens)-1) # # keep last piece of tokenized term -->> gives better results!\n",
        "    segment_ids.append(0)\n",
        "tokens.append(\"[SEP]\")\n",
        "segment_ids.append(0)\n",
        "orig_to_tok_map.append(len(tokens)-1)\n",
        "input_ids = tokenizer.convert_tokens_to_ids([tokens[i] for i in orig_to_tok_map])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpY1Oc5-u8HV",
        "outputId": "ef37d41b-8180-4c52-c5c0-f97a05f5e2a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "# see Tokenization\n",
        "\n",
        "#print('Original tokens:',tokens_a)\n",
        "#print('BERT tokens:',tokens)\n",
        "\n",
        "#orig_to_tok_map\n",
        "\n",
        "[#tokens[i] for i in orig_to_tok_map]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original tokens: ['plm', ':', 'poorly', 'digitized', 'labs', 'give', 'dassault', 'systèmes', '’', 'biovia', 'a', 'great', 'growth', 'potential']\n",
            "BERT tokens: ['[CLS]', 'pl', '##m', ':', 'poorly', 'digit', '##ized', 'labs', 'give', 'das', '##sau', '##lt', 'system', '##es', '’', 'bio', '##via', 'a', 'great', 'growth', 'potential', '[SEP]']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " '##m',\n",
              " ':',\n",
              " 'poorly',\n",
              " '##ized',\n",
              " 'labs',\n",
              " 'give',\n",
              " '##lt',\n",
              " '##es',\n",
              " '’',\n",
              " '##via',\n",
              " 'a',\n",
              " 'great',\n",
              " 'growth',\n",
              " 'potential',\n",
              " '[SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWf0oHo2vE1s"
      },
      "source": [
        "# Convert data to InputExample format\n",
        "test_example = convert_text_to_examples([sentence_ini], [['-PAD-']*len(sentence_ini)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8DS-OYIvH9r",
        "outputId": "672bc56f-504a-4aaa-dcbd-813fd17d3785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "fd4449d92f92409fb3a4fe7f3f8851e6",
            "1157c922482141dea7c60e627f2ca8fa",
            "59ea01088e6446979aa1077cff61e779",
            "3d1aac18764b4cbb9b740598ae569e02",
            "a159f42787f5410bb791a1800bc04900",
            "1f965885d7d9403dbdb09b349a6fad7d",
            "0a5d77d3898748ca907954e56e0bca8d",
            "066478ff4eeb4cdeb68b62c2393a138f"
          ]
        }
      },
      "source": [
        "# Convert to features\n",
        "(input_ids, input_masks, segment_ids, _\n",
        ") = convert_examples_to_features(tokenizer, test_example, max_seq_length=MAX_SEQUENCE_LENGTH+2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:108: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd4449d92f92409fb3a4fe7f3f8851e6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Converting examples to features', max=1.0, style=Progress…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrvzN-JQmlpP"
      },
      "source": [
        "## Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSJl6mgevLQ2",
        "outputId": "2deed7ee-a07f-4337-b21a-245ce34acd06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "# Print output\n",
        "\n",
        "predictions = model.predict([input_ids, input_masks, segment_ids], batch_size=1).argmax(-1)[0]\n",
        "print(\"\\n{:20}| {:15}: {:15}\".format(\"Word in BERT layer\", 'Initial word', \"Predicted POS-tag\"))\n",
        "print(61*'-')\n",
        "k = 0\n",
        "for i, pred in enumerate(predictions):\n",
        "    try:\n",
        "        if pred!=0:\n",
        "            print(\"{:20}| {:15}: {:15}\".format([tokens[i] for i in orig_to_tok_map][i], sentence_ini[i-1], int2tag[pred]))            \n",
        "            k+=1            \n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Word in BERT layer  | Initial word   : Predicted POS-tag\n",
            "-------------------------------------------------------------\n",
            "##m                 | plm            : PROPN          \n",
            ":                   | :              : PUNCT          \n",
            "poorly              | poorly         : ADV            \n",
            "##ized              | digitized      : VERB           \n",
            "labs                | labs           : NOUN           \n",
            "give                | give           : VERB           \n",
            "##lt                | dassault       : PROPN          \n",
            "##es                | systèmes       : PROPN          \n",
            "’                   | ’              : PUNCT          \n",
            "##via               | biovia         : NOUN           \n",
            "a                   | a              : DET            \n",
            "great               | great          : ADJ            \n",
            "growth              | growth         : NOUN           \n",
            "potential           | potential      : NOUN           \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}