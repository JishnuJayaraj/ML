{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNU3qNxhmSINcjDzughrWhN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JishnuJayaraj/ML/blob/master/NLP/Bert/summarization/BertSummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMfpx_9mNofG"
      },
      "source": [
        "# SUMMARIZATION AND NER\n",
        "!pip install transformers\n",
        "# https://chriskhanhtran.github.io/posts/named-entity-recognition-with-transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7Om6dUiDZ3W"
      },
      "source": [
        "# 1. Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-H4XCqQG5c_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmHhIGLSO_Uj"
      },
      "source": [
        "import pandas as pd \n",
        "import transformers\n",
        "#from transformers import DistilBertTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_liABqUPMT-"
      },
      "source": [
        "df = pd.read_csv('article_20.csv')\n",
        "\n",
        "# name the columns\n",
        "df.columns = ['rowno','date','heading','full_content','link','empty']\n",
        "\n",
        "# cut down columns\n",
        "articles = df[['heading','full_content']]\n",
        "articles['word_count'] = articles['full_content'].apply(lambda x: len(str(x).split(\" \")))\n",
        "articles['content'] = articles['full_content'].str.slice(0,1024)\n",
        "#articles['content4k'] = articles['full_content'].str.slice(0,4096)\n",
        "\n",
        "articles.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu7SAxL0RMVo"
      },
      "source": [
        "# Defining DistilBERT tokonizer\n",
        "distil_bert = 'distilbert-base-uncased' # Pick any desired pre-trained model\n",
        "roberta = 'roberta-base-uncase'\n",
        "\n",
        "# change name here to change tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,\n",
        "                                                max_length=512, pad_to_max_length=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXMO59SyaWUN"
      },
      "source": [
        "# Tokenize the document\n",
        "def tokenize(sentences, tokenizer):\n",
        "    input_ids, input_masks, input_segments = [],[],[]\n",
        "    for sentence in tqdm(sentences):\n",
        "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=128, pad_to_max_length=True, \n",
        "                                             return_attention_mask=True, return_token_type_ids=True)\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])        \n",
        "        \n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN39UWaFcGS5"
      },
      "source": [
        "## Training\n",
        "* Use Pretrained model directly as a classifier\n",
        "* Transformer model to extract embedding and use it as input to another classifier.\n",
        "* Fine-tuning a Pretrained transformer model on custom config and dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtS009PscIto"
      },
      "source": [
        "# SUMMARIZATION\n",
        "from transformers import pipeline\n",
        "\n",
        "#summarizer = pipeline(\"summarization\")\n",
        "summarizer = pipeline('summarization', model='bart-large-cnn', tokenizer='bart-large-cnn')\n",
        "\n",
        "#from __future__ import print_function\n",
        "#import ipywidgets as widgets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhrBE2gAB9NR"
      },
      "source": [
        "#summarizer(article, max_length=90, min_length=20)\n",
        "articles['summary'] = articles['content'].apply(summarizer)\n",
        "#articles['summary'] = articles['content4k'].apply(summarizer)\n",
        "#articles = articles[['heading','full_content','word_count','content4k','summary']]\n",
        "#articles.to_csv('final.csv')\n",
        "#print(summary[0]['summary_text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKL_7luHK5Sg"
      },
      "source": [
        "# New Hugging face pipeline summarizer has a input text limit\n",
        "# recreating a old summarizer\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, AutoTokenizer, AutoModel\n",
        "from typing import List\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\t\t\t\n",
        "model = AutoModel.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "   \n",
        "def old_summarization_pipeline(text: List[str]) -> List[str]:\n",
        "    #tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')\n",
        "    #model = BartForConditionalGeneration.from_pretrained('bart-large-cnn')\n",
        "    input_ids = tokenizer.batch_encode_plus(text, return_tensors='pt', max_length=1024)['input_ids']\n",
        "    summary_ids = model.generate(input_ids)\n",
        "    summaries = [tokenizer.decode(s) for s in summary_ids]\n",
        "    return summaries\n",
        "\n",
        "# text = '=' * 10257  \n",
        "text = \"In a world where distributed product development and manufacturing are constantly gaining ground, collaboration capabilities are one of the factors that are key to success. The better the tools you have that can make extended enterprise collaboration easier, the more effectively the marketing, development and production work can be executed. What follows from this is broader collaborative flexibility, shorter time to market and other valuable advantages. Any of the big PLM systems have solutions for this, but just how effective are these collaboration tools in terms of a third-party non-engineering team involved in the product realization? These days, it’s not uncommon to work with teams and other people outside the engineering departments. Do they have access to the needed tools, such as those from the big PLM players Dassault Systémes, PTC or Siemens?\"\n",
        "old_summarization_pipeline(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQYdlDGRf8b7"
      },
      "source": [
        "# NER\n",
        "\n",
        "ner_tokenizer = pipeline(\"ner\")\n",
        "text = 'Jishnu is born in Kerala. He like to travel and eat alot. His favouite meal is chicken and soup. He works in Microsoft in California from this December'\n",
        "\n",
        "#print(ner_tokenizer(art1))\n",
        "print('************')\n",
        "print(ner_tokenizer(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1fOmUQqG7Lq"
      },
      "source": [
        "# BERT-NER from depend on defnition\n",
        "[Mccormik fine tuning: Glue](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
        "\n",
        "[depends on def](https://www.depends-on-the-definition.com/named-entity-recognition-with-bert/)\n",
        "\n",
        "[medium: painless fine Tuning](https://medium.com/swlh/painless-fine-tuning-of-bert-in-pytorch-b91c14912caa)"
      ]
    }
  ]
}