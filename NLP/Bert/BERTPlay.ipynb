{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTPlay.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_Y5mJ8gZljAh"
      ],
      "toc_visible": true,
      "mount_file_id": "11Of8s6XnTR25UBGJib4dvR-sQsoG-r9k",
      "authorship_tag": "ABX9TyMhXU1WuWjmcHoY1dJMuBOq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JishnuJayaraj/ML/blob/master/NLP/Bert/BERTPlay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuIPFfjlVjNd"
      },
      "source": [
        "# Junk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLhkAO3VUzmO"
      },
      "source": [
        "# SUMMARIZATION AND NER\n",
        "!pip install transformers\n",
        "# https://chriskhanhtran.github.io/posts/named-entity-recognition-with-transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKSBD7Ruma07"
      },
      "source": [
        "input text -> load model with pretrained weights -> tokenize to byte seq: Bert tokenizer -> cls, sep token -> padding: max_length -> masking tokens\n",
        "\n",
        "-> tokens to bert vocab ->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Wbov-e9ONQ4"
      },
      "source": [
        "## Sub word Tokenization\n",
        "words cat and cats, a sub-tokenization of the word cats would be [cat, ##s]. Where the prefix \"##\" indicates a subtoken of the initial input. Such training algorithms might extract sub-tokens such as \"##ing\", \"##ed\" over English corpus.\n",
        "\n",
        "this kind of sub-tokens construction leveraging compositions of \"pieces\" overall reduces the size of the vocabulary you have to carry to train a Machine Learning model. On the other side, as one token might be exploded into multiple subtokens, the input of your model might increase and become an issue on model with non-linear complexity over the input sequence's length.\n",
        "\n",
        "### huggingface/tokenizers library\n",
        "blazing fast tokenization library able to train, tokenize and decode dozens of Gb/s of text on a common multi-core machine.\n",
        "library so that it provides all the required blocks to create end-to-end tokenizers in an interchangeable way. In that sense, we provide these various components:\n",
        "\n",
        "* **Normalizer**: Executes all the initial transformations over the initial input string. For example when you need to lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer.\n",
        "* PreTokenizer: In charge of splitting the initial input string. That's the component that decides where and how to pre-segment the origin string. The simplest example would be like we saw before, to simply split on spaces.\n",
        "* Model: Handles all the sub-token discovery and generation, this part is trainable and really dependant of your input data.\n",
        "* Post-Processor: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.\n",
        "* Decoder: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according to the PreTokenizer we used previously.\n",
        "* Trainer: Provides training capabilities to each model.\n",
        "\n",
        "For each of the components above we provide multiple implementations:\n",
        "\n",
        "* Normalizer: Lowercase, Unicode (NFD, NFKD, NFC, NFKC), Bert, Strip, ...\n",
        "* PreTokenizer: ByteLevel, WhitespaceSplit, CharDelimiterSplit, Metaspace, ...\n",
        "* Model: WordLevel, BPE, WordPiece\n",
        "* Post-Processor: BertProcessor, ...\n",
        "* Decoder: WordLevel, BPE, WordPiece, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiXbCgcvU8i4"
      },
      "source": [
        "import pandas as pd \n",
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4HrlVPKWP2_"
      },
      "source": [
        "from tokenizers import Tokenizer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC-uvviIcxGX"
      },
      "source": [
        "## Transformer library\n",
        "The transformers library allows you to benefits from large, pretrained language models without requiring a huge and costly computational infrastructure. Most of the State-of-the-Art models are provided directly by their author and made available in the library in PyTorch and TensorFlow in a transparent and interchangeable way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClWZvaB4d19s",
        "outputId": "a934851d-281a-4f7c-eb71-69f1cc27d788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "e9257cb6096c4d0f9a0cb46d8dd6824a",
            "7c6b7ece04e64fa0a92f8913aa7b3ca3",
            "253201b2f3d04687b8c9c9b195be8ee8",
            "fcd3adf32abb4028b992eb2615581584",
            "a74d947c159a4f3088cbb5011af40212",
            "5e0b457a18214ad2a353faae6941fe90",
            "1a9e0d085a0c4273bb7a71fe183487b2",
            "7bbcae3c0c4145cc911afb516ed517cc",
            "2f1e45b6a89e44f3a40ee233b9593deb",
            "d1acabf47c97414ba19807b538861e46",
            "79c281ce39ef41c6b4e54d986b45b92a",
            "f479849939464ba99e748b710c01ff57",
            "06b858251c124449959597c86f634407",
            "de47ec1e664142a889d263f124b0bf88",
            "f6c86594a5754744a396513b9d87c574",
            "922e076d8abb45d3b30b3d39ae50debc",
            "927600bf395547beb09ed22685493a81",
            "2c94586bc83a4569b4b99a019b6ac009",
            "f89f68dc214f49689cf0b54b9c5a008a",
            "4e6dc3d80c0d447b8b9dff5f058c60aa",
            "af5bd6b1eb054279ad9f4293c9bf66b6",
            "bd8620fff12e412db378fc6b2f30a1f7",
            "844e21b5d2ea4bf28056a453257317c8",
            "1feeabf46f0c420784e0a8b64e952e68"
          ]
        }
      },
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Store the model we want to use\n",
        "MODEL_NAME = \"bert-base-cased\"\n",
        "\n",
        "# We need to create the model and tokenizer\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Tokens comes from a process that splits the input into sub-entities with interesting linguistic properties. \n",
        "tokens = tokenizer.tokenize(\"This is an input example\")\n",
        "print(\"Tokens: {}\".format(tokens))\n",
        "\n",
        "# This is not sufficient for the model, as it requires integers as input, \n",
        "# not a problem, let's convert tokens to ids.\n",
        "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Tokens id: {}\".format(tokens_ids))\n",
        "\n",
        "# Add the required special tokens\n",
        "tokens_ids = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
        "\n",
        "# We need to convert to a Deep Learning framework specific format, let's use PyTorch for now.\n",
        "tokens_pt = torch.tensor([tokens_ids])\n",
        "print(\"Tokens PyTorch: {}\".format(tokens_pt))\n",
        "\n",
        "# Now we're ready to go through BERT with out input\n",
        "outputs, pooled = model(tokens_pt)\n",
        "print(\"Token wise output: {}, Pooled output: {}\".format(outputs.shape, pooled.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e9257cb6096c4d0f9a0cb46d8dd6824a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f1e45b6a89e44f3a40ee233b9593deb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435779157.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "927600bf395547beb09ed22685493a81",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Tokens: ['This', 'is', 'an', 'input', 'example']\n",
            "Tokens id: [1188, 1110, 1126, 7758, 1859]\n",
            "Tokens PyTorch: tensor([[ 101, 1188, 1110, 1126, 7758, 1859,  102]])\n",
            "Token wise output: torch.Size([1, 7, 768]), Pooled output: torch.Size([1, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XVAZYEnVfVu"
      },
      "source": [
        "As you can see, BERT outputs two tensors:\n",
        "\n",
        "* One with the generated representation for every token in the input (1, NB_TOKENS, REPRESENTATION_SIZE)\n",
        "* One with an aggregated representation for the whole input (1, REPRESENTATION_SIZE)\n",
        "\n",
        "The first, token-based, representation can be leveraged if your task requires to keep the sequence representation and you want to operate at a token-level. This is particularly useful for Named Entity Recognition and Question-Answering.\n",
        "\n",
        "The second, aggregated, representation is especially useful if you need to extract the overall context of the sequence and don't require a fine-grained token-level. This is the case for Sentiment-Analysis of the sequence or Information Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfr4AclbWO0P",
        "outputId": "c40606ac-9aaa-4d16-d9e3-09e29b82b0ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# high level \n",
        "# tokens = tokenizer.tokenize(\"This is an input example\")\n",
        "# tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "# tokens_pt = torch.tensor([tokens_ids])\n",
        "\n",
        "# This code can be factored into one-line as follow\n",
        "tokens_pt2 = tokenizer.encode_plus(\"This is an input example\", return_tensors=\"pt\")\n",
        "\n",
        "for key, value in tokens_pt2.items():\n",
        "    print(\"{}:\\n\\t{}\".format(key, value))\n",
        "\n",
        "outputs2, pooled2 = model(**tokens_pt2)\n",
        "print(\"Difference with previous code: ({}, {})\".format((outputs2 - outputs).sum(), (pooled2 - pooled).sum()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_ids:\n",
            "\ttensor([[ 101, 1188, 1110, 1126, 7758, 1859,  102]])\n",
            "token_type_ids:\n",
            "\ttensor([[0, 0, 0, 0, 0, 0, 0]])\n",
            "attention_mask:\n",
            "\ttensor([[1, 1, 1, 1, 1, 1, 1]])\n",
            "Difference with previous code: (0.0, 0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_LFoQiKWUsM"
      },
      "source": [
        "\n",
        "As you can see above, the methode encode_plus provides a convenient way to generate all the required parameters that will go through the model.\n",
        "\n",
        "Moreover, you might have noticed it generated some additional tensors:\n",
        "\n",
        "token_type_ids: This tensor will map every tokens to their corresponding segment (see below).\n",
        "attention_mask: This tensor is used to \"mask\" padded values in a batch of sequence with different lengths (see below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V3G5E7AWoY_"
      },
      "source": [
        "# Padding highlight\n",
        "tokens = tokenizer.batch_encode_plus(\n",
        "    [\"This is a sample\", \"This is another longer sample text\"], \n",
        "    pad_to_max_length=True  # First sentence will have some PADDED tokens to match second sequence length\n",
        ")\n",
        "\n",
        "for i in range(2):\n",
        "    print(\"Tokens (int)      : {}\".format(tokens['input_ids'][i]))\n",
        "    print(\"Tokens (str)      : {}\".format([tokenizer.convert_ids_to_tokens(s) for s in tokens['input_ids'][i]]))\n",
        "    print(\"Tokens (attn_mask): {}\".format(tokens['attention_mask'][i]))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1abnn_M-Wt1g"
      },
      "source": [
        "# DISTIL BERT\n",
        "\n",
        "from transformers import DistilBertModel\n",
        "\n",
        "bert_distil = DistilBertModel.from_pretrained('distilbert-base-cased')\n",
        "input_pt = tokenizer.encode_plus(\n",
        "    'This is a sample input to demonstrate performance of distiled models especially inference time', \n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "\n",
        "%time _ = bert_distil(input_pt['input_ids'])\n",
        "# %time _ = model_pt(input_pt['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OS6o8qNkYJBP"
      },
      "source": [
        "## HOW TO USE PIPELINES\n",
        "\n",
        "Newly introduced in transformers v2.3.0, pipelines provides a high-level, easy to use, API for doing inference over a variety of downstream-tasks, including:\n",
        "\n",
        "* Sentence Classification (Sentiment Analysis): Indicate if the overall sentence is either positive or negative, i.e. binary classification task or logitic regression task.\n",
        "* Token Classification (Named Entity Recognition, Part-of-Speech tagging): For each sub-entities (tokens) in the input, assign them a label, i.e. classification task.\n",
        "* Question-Answering: Provided a tuple (question, context) the model should find the span of text in content answering the question.\n",
        "* Mask-Filling: Suggests possible word(s) to fill the masked input with respect to the provided context.\n",
        "* Summarization: Summarizes the input article to a shorter article.\n",
        "* Translation: Translates the input from a language to another language.\n",
        "* Feature Extraction: Maps the input to a higher, multi-dimensional space learned from the data.\n",
        "\n",
        "\n",
        "Pipelines encapsulate the overall process of every NLP process:\n",
        "\n",
        "* Tokenization: Split the initial input into multiple sub-entities with ... properties (i.e. tokens).\n",
        "* Inference: Maps every tokens into a more meaningful representation.\n",
        "* Decoding: Use the above representation to generate and/or extract the final output for the underlying task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pggItQkBW3rc"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "nlp_token_class = pipeline('ner')\n",
        "nlp_token_class('Hugging Face is a French company based in New-York.')\n",
        "\n",
        "TEXT_TO_SUMMARIZE = \"\"\" \n",
        "New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York. \n",
        "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband. \n",
        "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometime\"\"\"\n",
        "\n",
        "summarizer = pipeline('summarization')\n",
        "summarizer(TEXT_TO_SUMMARIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw2_XHblU_Kb"
      },
      "source": [
        "df = pd.read_csv('article_20.csv')\n",
        "\n",
        "# name the columns\n",
        "df.columns = ['rowno','date','heading','full_content','link','empty']\n",
        "\n",
        "# cut down columns\n",
        "articles = df[['heading','full_content']]\n",
        "articles['word_count'] = articles['full_content'].apply(lambda x: len(str(x).split(\" \")))\n",
        "articles['content'] = articles['full_content'].str.slice(0,1024)\n",
        "#articles['content4k'] = articles['full_content'].str.slice(0,4096)\n",
        "\n",
        "articles.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iaCoRmQVEXw"
      },
      "source": [
        "# Defining DistilBERT tokonizer\n",
        "distil_bert = 'distilbert-base-uncased' # Pick any desired pre-trained model\n",
        "roberta = 'roberta-base-uncase'\n",
        "\n",
        "# change name here to change tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,\n",
        "                                                max_length=512, pad_to_max_length=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BXuenLqVIcY"
      },
      "source": [
        "# Tokenize the document\n",
        "def tokenize(sentences, tokenizer):\n",
        "    input_ids, input_masks, input_segments = [],[],[]\n",
        "    for sentence in tqdm(sentences):\n",
        "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=128, pad_to_max_length=True, \n",
        "                                             return_attention_mask=True, return_token_type_ids=True)\n",
        "        input_ids.append(inputs['input_ids'])\n",
        "        input_masks.append(inputs['attention_mask'])\n",
        "        input_segments.append(inputs['token_type_ids'])        \n",
        "        \n",
        "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}