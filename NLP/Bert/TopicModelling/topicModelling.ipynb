{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topicModelling.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMu/CcFmKDBGQDQD+MUJuc1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JishnuJayaraj/ML/blob/master/NLP/Bert/TopicModelling/topicModelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peA_7rM3SuPb"
      },
      "source": [
        "#Bert topic modelling\n",
        "\n",
        "\n",
        "https://github.com/huseinzol05/NLP-Models-Tensorflow/blob/master/topic-model/2.bert-topic.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpGr8S1-SvpG",
        "outputId": "eb40f489-b251-4803-cf7e-cda76bbe87be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "\n",
        "# download BERT base model\n",
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
        "!unzip cased_L-12_H-768_A-12.zip\n",
        "\n",
        "#download simple data set  ------>\n",
        "!wget https://raw.githubusercontent.com/huseinzol05/NLP-Models-Tensorflow/master/text-classification/data/negative/negative\n",
        "\n",
        "with open('negative') as fopen:\n",
        "    negative = fopen.read().split('\\n')[:-1]\n",
        "len(negative)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-10 15:46:30--  https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.204.128, 172.217.203.128, 173.194.214.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 404261442 (386M) [application/zip]\n",
            "Saving to: ‘cased_L-12_H-768_A-12.zip.1’\n",
            "\n",
            "cased_L-12_H-768_A- 100%[===================>] 385.53M   155MB/s    in 2.5s    \n",
            "\n",
            "2020-07-10 15:46:33 (155 MB/s) - ‘cased_L-12_H-768_A-12.zip.1’ saved [404261442/404261442]\n",
            "\n",
            "Archive:  cased_L-12_H-768_A-12.zip\n",
            "replace cased_L-12_H-768_A-12/bert_model.ckpt.meta? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace cased_L-12_H-768_A-12/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace cased_L-12_H-768_A-12/bert_model.ckpt.index? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace cased_L-12_H-768_A-12/bert_config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "--2020-07-10 15:47:41--  https://raw.githubusercontent.com/huseinzol05/NLP-Models-Tensorflow/master/text-classification/data/negative/negative\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 574425 (561K) [text/plain]\n",
            "Saving to: ‘negative.1’\n",
            "\n",
            "negative.1          100%[===================>] 560.96K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2020-07-10 15:47:42 (6.10 MB/s) - ‘negative.1’ saved [574425/574425]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5330"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vrM9-ZpTJcH",
        "outputId": "ed31cc7f-b015-4c0c-ca76-32f358291b0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# !pip uninstall tensorflow==2.2.0\n",
        "# !pip install tensorflow==1.15.0\n",
        "# !pip install bert-tensorflow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n",
            "Uninstalling tensorflow-2.2.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-2.2.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "y\n",
            "yes\n",
            "Y\n",
            "  Successfully uninstalled tensorflow-2.2.0\n",
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 34kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.2.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.30.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.34.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.18.5)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.9.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 37.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (47.3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=0fd67c6fee9baa62f4283b663d5cd2c75d52775b535d8ef56c82cad2d829eee2\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-tensorflow in /usr/local/lib/python3.6/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6712f2c29f0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bert/run_classifier.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bert/optimization.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAdamWeightDecayOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m   \u001b[0;34m\"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'Optimizer'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1APNAh4YeMC",
        "outputId": "0a8548d2-3d70-46e1-fc15-9212623c4c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n",
        "from bert import modeling\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9AqCao8TcPh"
      },
      "source": [
        "BERT_VOCAB = 'cased_L-12_H-768_A-12/vocab.txt'\n",
        "BERT_INIT_CHKPNT = 'cased_L-12_H-768_A-12/bert_model.ckpt'\n",
        "BERT_CONFIG = 'cased_L-12_H-768_A-12/bert_config.json'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbYkuuPsY-SB"
      },
      "source": [
        "# functions\n",
        "def generate_ngram(seq, ngram = (1, 3)):\n",
        "    g = []\n",
        "    for i in range(ngram[0], ngram[-1] + 1):\n",
        "        g.extend(list(ngrams_generator(seq, i)))\n",
        "    return g\n",
        "\n",
        "def _pad_sequence(\n",
        "    sequence,\n",
        "    n,\n",
        "    pad_left = False,\n",
        "    pad_right = False,\n",
        "    left_pad_symbol = None,\n",
        "    right_pad_symbol = None,\n",
        "):\n",
        "    sequence = iter(sequence)\n",
        "    if pad_left:\n",
        "        sequence = itertools.chain((left_pad_symbol,) * (n - 1), sequence)\n",
        "    if pad_right:\n",
        "        sequence = itertools.chain(sequence, (right_pad_symbol,) * (n - 1))\n",
        "    return sequence\n",
        "\n",
        "\n",
        "def ngrams_generator(\n",
        "    sequence,\n",
        "    n,\n",
        "    pad_left = False,\n",
        "    pad_right = False,\n",
        "    left_pad_symbol = None,\n",
        "    right_pad_symbol = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    generate ngrams.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    sequence : list of str\n",
        "        list of tokenize words.\n",
        "    n : int\n",
        "        ngram size\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ngram: list\n",
        "    \"\"\"\n",
        "    sequence = _pad_sequence(\n",
        "        sequence, n, pad_left, pad_right, left_pad_symbol, right_pad_symbol\n",
        "    )\n",
        "\n",
        "    history = []\n",
        "    while n > 1:\n",
        "        try:\n",
        "            next_item = next(sequence)\n",
        "        except StopIteration:\n",
        "            return\n",
        "        history.append(next_item)\n",
        "        n -= 1\n",
        "    for item in sequence:\n",
        "        history.append(item)\n",
        "        yield tuple(history)\n",
        "        del history[0]\n",
        "\n",
        "def merge_wordpiece_tokens(paired_tokens, weighted = True):\n",
        "    new_paired_tokens = []\n",
        "    n_tokens = len(paired_tokens)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    while i < n_tokens:\n",
        "        current_token, current_weight = paired_tokens[i]\n",
        "        if current_token.startswith('##'):\n",
        "            previous_token, previous_weight = new_paired_tokens.pop()\n",
        "            merged_token = previous_token\n",
        "            merged_weight = [previous_weight]\n",
        "            while current_token.startswith('##'):\n",
        "                merged_token = merged_token + current_token.replace('##', '')\n",
        "                merged_weight.append(current_weight)\n",
        "                i = i + 1\n",
        "                current_token, current_weight = paired_tokens[i]\n",
        "            merged_weight = np.mean(merged_weight)\n",
        "            new_paired_tokens.append((merged_token, merged_weight))\n",
        "\n",
        "        else:\n",
        "            new_paired_tokens.append((current_token, current_weight))\n",
        "            i = i + 1\n",
        "\n",
        "    words = [\n",
        "        i[0]\n",
        "        for i in new_paired_tokens\n",
        "        if i[0] not in ['[CLS]', '[SEP]', '[PAD]']\n",
        "    ]\n",
        "    weights = [\n",
        "        i[1]\n",
        "        for i in new_paired_tokens\n",
        "        if i[0] not in ['[CLS]', '[SEP]', '[PAD]']\n",
        "    ]\n",
        "    if weighted:\n",
        "        weights = np.array(weights)\n",
        "        weights = weights / np.sum(weights)\n",
        "    return list(zip(words, weights))\n",
        "\n",
        "def _extract_attention_weights(num_layers, tf_graph):\n",
        "    attns = [\n",
        "        {\n",
        "            'layer_%s'\n",
        "            % i: tf_graph.get_tensor_by_name(\n",
        "                'bert/encoder/layer_%s/attention/self/Softmax:0' % i\n",
        "            )\n",
        "        }\n",
        "        for i in range(num_layers)\n",
        "    ]\n",
        "\n",
        "    return attns\n",
        "\n",
        "def padding_sequence(seq, maxlen, padding = 'post', pad_int = 0):\n",
        "    padded_seqs = []\n",
        "    for s in seq:\n",
        "        if padding == 'post':\n",
        "            padded_seqs.append(s + [pad_int] * (maxlen - len(s)))\n",
        "        if padding == 'pre':\n",
        "            padded_seqs.append([pad_int] * (maxlen - len(s)) + s)\n",
        "    return padded_seqs\n",
        "\n",
        "\n",
        "def bert_tokenization(tokenizer, texts, cls = '[CLS]', sep = '[SEP]'):\n",
        "\n",
        "    input_ids, input_masks, segment_ids, s_tokens = [], [], [], []\n",
        "    for text in texts:\n",
        "        tokens_a = tokenizer.tokenize(text)\n",
        "        tokens = [cls] + tokens_a + [sep]\n",
        "        segment_id = [0] * len(tokens)\n",
        "        input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_mask = [1] * len(input_id)\n",
        "\n",
        "        input_ids.append(input_id)\n",
        "        input_masks.append(input_mask)\n",
        "        segment_ids.append(segment_id)\n",
        "        s_tokens.append(tokens)\n",
        "\n",
        "    maxlen = max([len(i) for i in input_ids])\n",
        "    input_ids = padding_sequence(input_ids, maxlen)\n",
        "    input_masks = padding_sequence(input_masks, maxlen)\n",
        "    segment_ids = padding_sequence(segment_ids, maxlen)\n",
        "\n",
        "    return input_ids, input_masks, segment_ids, s_tokens\n",
        "\n",
        "class _Model:\n",
        "    def __init__(self, bert_config, tokenizer):\n",
        "        _graph = tf.Graph()\n",
        "        with _graph.as_default():\n",
        "            self.X = tf.placeholder(tf.int32, [None, None])\n",
        "            self._tokenizer = tokenizer\n",
        "\n",
        "            self.model = modeling.BertModel(\n",
        "                config = bert_config,\n",
        "                is_training = False,\n",
        "                input_ids = self.X,\n",
        "                use_one_hot_embeddings = False,\n",
        "            )\n",
        "            self.logits = self.model.get_pooled_output()\n",
        "            self._sess = tf.InteractiveSession()\n",
        "            self._sess.run(tf.global_variables_initializer())\n",
        "            var_lists = tf.get_collection(\n",
        "                tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'bert'\n",
        "            )\n",
        "            self._saver = tf.train.Saver(var_list = var_lists)\n",
        "            attns = _extract_attention_weights(\n",
        "                bert_config.num_hidden_layers, tf.get_default_graph()\n",
        "            )\n",
        "            self.attns = attns\n",
        "\n",
        "    def vectorize(self, strings):\n",
        "\n",
        "        \"\"\"\n",
        "        Vectorize string inputs using bert attention.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        strings : str / list of str\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        array: vectorized strings\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(strings, list):\n",
        "            if not isinstance(strings[0], str):\n",
        "                raise ValueError('input must be a list of strings or a string')\n",
        "        else:\n",
        "            if not isinstance(strings, str):\n",
        "                raise ValueError('input must be a list of strings or a string')\n",
        "        if isinstance(strings, str):\n",
        "            strings = [strings]\n",
        "\n",
        "        batch_x, _, _, _ = bert_tokenization(self._tokenizer, strings)\n",
        "        return self._sess.run(self.logits, feed_dict = {self.X: batch_x})\n",
        "\n",
        "    def attention(self, strings, method = 'last', **kwargs):\n",
        "        \"\"\"\n",
        "        Get attention string inputs from bert attention.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        strings : str / list of str\n",
        "        method : str, optional (default='last')\n",
        "            Attention layer supported. Allowed values:\n",
        "\n",
        "            * ``'last'`` - attention from last layer.\n",
        "            * ``'first'`` - attention from first layer.\n",
        "            * ``'mean'`` - average attentions from all layers.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        array: attention\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(strings, list):\n",
        "            if not isinstance(strings[0], str):\n",
        "                raise ValueError('input must be a list of strings or a string')\n",
        "        else:\n",
        "            if not isinstance(strings, str):\n",
        "                raise ValueError('input must be a list of strings or a string')\n",
        "        if isinstance(strings, str):\n",
        "            strings = [strings]\n",
        "\n",
        "        method = method.lower()\n",
        "        if method not in ['last', 'first', 'mean']:\n",
        "            raise Exception(\n",
        "                \"method not supported, only support 'last', 'first' and 'mean'\"\n",
        "            )\n",
        "\n",
        "        batch_x, _, _, s_tokens = bert_tokenization(self._tokenizer, strings)\n",
        "        maxlen = max([len(s) for s in s_tokens])\n",
        "        s_tokens = padding_sequence(s_tokens, maxlen, pad_int = '[SEP]')\n",
        "        attentions = self._sess.run(self.attns, feed_dict = {self.X: batch_x})\n",
        "        if method == 'first':\n",
        "            cls_attn = list(attentions[0].values())[0][:, :, 0, :]\n",
        "\n",
        "        if method == 'last':\n",
        "            cls_attn = list(attentions[-1].values())[0][:, :, 0, :]\n",
        "\n",
        "        if method == 'mean':\n",
        "            combined_attentions = []\n",
        "            for a in attentions:\n",
        "                combined_attentions.append(list(a.values())[0])\n",
        "            cls_attn = np.mean(combined_attentions, axis = 0).mean(axis = 2)\n",
        "\n",
        "        cls_attn = np.mean(cls_attn, axis = 1)\n",
        "        total_weights = np.sum(cls_attn, axis = -1, keepdims = True)\n",
        "        attn = cls_attn / total_weights\n",
        "        output = []\n",
        "        for i in range(attn.shape[0]):\n",
        "            output.append(\n",
        "                merge_wordpiece_tokens(list(zip(s_tokens[i], attn[i])))\n",
        "            )\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbVhXJgBZTkU"
      },
      "source": [
        "tokenizer = tokenization.FullTokenizer(vocab_file=BERT_VOCAB, do_lower_case=False)\n",
        "bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
        "model = _Model(bert_config, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjkIkKUTZb8I",
        "outputId": "d7e1c2a4-4d55-4a8a-9626-5596b0129e0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# test vectorization\n",
        "v = model.vectorize(['hello nice to meet u', 'so long sucker'])\n",
        "v.shape\n",
        "v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.4648218 , -0.13978343, -0.64823234, ...,  0.45981765,\n",
              "         0.34414184, -0.4053322 ],\n",
              "       [ 0.46562865, -0.12565361, -0.5543854 , ...,  0.49884537,\n",
              "         0.18823244, -0.26777568]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-iRcMLmZjQ4",
        "outputId": "df39432b-012d-484b-d7a5-62cad9162dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#test attention\n",
        "model.attention(['hello nice to meet u', 'so long sucker'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('hello', 0.19973749),\n",
              "  ('nice', 0.19126925),\n",
              "  ('to', 0.18114448),\n",
              "  ('meet', 0.21839985),\n",
              "  ('u', 0.20944896)],\n",
              " [('so', 0.32317767), ('long', 0.35368302), ('sucker', 0.32313925)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-Fyu4iIZuGx"
      },
      "source": [
        "# Build topic moelling\n",
        "batch_size = 10\n",
        "ngram = (1, 3)\n",
        "n_topics = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgmd98C0Z6Cn",
        "outputId": "2a117a62-7e68-413f-cc6b-de7f96c118ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# ----->\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm import tqdm\n",
        "\n",
        "rows, attentions = [], []\n",
        "for i in tqdm(range(0, len(negative), batch_size)):\n",
        "    index = min(i + batch_size, len(negative))\n",
        "    rows.append(model.vectorize(negative[i:index]))\n",
        "    attentions.extend(model.attention(negative[i:index]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 533/533 [29:47<00:00,  3.35s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQCxRcw5anqK",
        "outputId": "35809205-c34f-4302-b18d-f8ff2987fc2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# stop words\n",
        "!wget https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.json\n",
        "\n",
        "import json\n",
        "with open('stopwords-en.json') as fopen:\n",
        "    stopwords = json.load(fopen)\n",
        "len(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-10 16:20:15--  https://raw.githubusercontent.com/stopwords-iso/stopwords-en/master/stopwords-en.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10275 (10K) [text/plain]\n",
            "Saving to: ‘stopwords-en.json’\n",
            "\n",
            "\rstopwords-en.json     0%[                    ]       0  --.-KB/s               \rstopwords-en.json   100%[===================>]  10.03K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-07-10 16:20:15 (88.6 MB/s) - ‘stopwords-en.json’ saved [10275/10275]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1298"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhrc7MQda6iD",
        "outputId": "f4662bed-9395-4f15-b0dc-0809c83b735c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#TODO\n",
        "concat = np.concatenate(rows, axis = 0)\n",
        "kmeans = KMeans(n_clusters = n_topics, random_state = 0).fit(concat)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "overall, filtered_a = [], []\n",
        "for a in attentions:\n",
        "    f = [i for i in a if i[0] not in stopwords]\n",
        "    overall.extend(f)\n",
        "    filtered_a.append(f)\n",
        "\n",
        "o_ngram = generate_ngram(overall, ngram)\n",
        "features = []\n",
        "for i in o_ngram:\n",
        "    features.append(' '.join([w[0] for w in i]))\n",
        "features = list(set(features))\n",
        "\n",
        "components = np.zeros((n_topics, len(features)))\n",
        "for no, i in enumerate(labels):\n",
        "    if (no + 1) % 500 == 0:\n",
        "        print('processed %d'%(no + 1))\n",
        "    f = generate_ngram(filtered_a[no], ngram)\n",
        "    for w in f:\n",
        "        word = ' '.join([r[0] for r in w])\n",
        "        score = np.mean([r[1] for r in w])\n",
        "        if word in features:\n",
        "            components[i, features.index(word)] += score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processed 500\n",
            "processed 1000\n",
            "processed 1500\n",
            "processed 2000\n",
            "processed 2500\n",
            "processed 3000\n",
            "processed 3500\n",
            "processed 4000\n",
            "processed 4500\n",
            "processed 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XixKCU7a_K_",
        "outputId": "04982b81-591d-4bbd-8223-96c2e5c331c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "def print_topics_modelling(\n",
        "    topics, feature_names, sorting, n_words = 20, return_df = True\n",
        "):\n",
        "    if return_df:\n",
        "        try:\n",
        "            import pandas as pd\n",
        "        except:\n",
        "            raise Exception(\n",
        "                'pandas not installed. Please install it and try again or set `return_df = False`'\n",
        "            )\n",
        "    df = {}\n",
        "    for i in range(topics):\n",
        "        words = []\n",
        "        for k in range(n_words):\n",
        "            words.append(feature_names[sorting[i, k]])\n",
        "        df['topic %d' % (i)] = words\n",
        "    if return_df:\n",
        "        return pd.DataFrame.from_dict(df)\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "print_topics_modelling(\n",
        "    10,\n",
        "    feature_names = np.array(features),\n",
        "    sorting = np.argsort(components)[:, ::-1],\n",
        "    n_words = 10,\n",
        "    return_df = True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic 0</th>\n",
              "      <th>topic 1</th>\n",
              "      <th>topic 2</th>\n",
              "      <th>topic 3</th>\n",
              "      <th>topic 4</th>\n",
              "      <th>topic 5</th>\n",
              "      <th>topic 6</th>\n",
              "      <th>topic 7</th>\n",
              "      <th>topic 8</th>\n",
              "      <th>topic 9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>movie</td>\n",
              "      <td>movie</td>\n",
              "      <td>movie</td>\n",
              "      <td>movie</td>\n",
              "      <td>movie</td>\n",
              "      <td>movie</td>\n",
              "      <td>movie</td>\n",
              "      <td>movie</td>\n",
              "      <td>movie</td>\n",
              "      <td>movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>film</td>\n",
              "      <td>film</td>\n",
              "      <td>film</td>\n",
              "      <td>film</td>\n",
              "      <td>film</td>\n",
              "      <td>film</td>\n",
              "      <td>film</td>\n",
              "      <td>film</td>\n",
              "      <td>film</td>\n",
              "      <td>film</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bad</td>\n",
              "      <td>comedy</td>\n",
              "      <td>characters</td>\n",
              "      <td>story</td>\n",
              "      <td>characters</td>\n",
              "      <td>story</td>\n",
              "      <td>bad</td>\n",
              "      <td>story</td>\n",
              "      <td>bad</td>\n",
              "      <td>story</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>story</td>\n",
              "      <td>lame</td>\n",
              "      <td>time</td>\n",
              "      <td>bad</td>\n",
              "      <td>time</td>\n",
              "      <td>time</td>\n",
              "      <td>characters</td>\n",
              "      <td>time</td>\n",
              "      <td>action</td>\n",
              "      <td>comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>time</td>\n",
              "      <td>bad</td>\n",
              "      <td>comedy</td>\n",
              "      <td>comedy</td>\n",
              "      <td>story</td>\n",
              "      <td>bad</td>\n",
              "      <td>plot</td>\n",
              "      <td>director</td>\n",
              "      <td>comedy</td>\n",
              "      <td>bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>films</td>\n",
              "      <td>silly</td>\n",
              "      <td>action</td>\n",
              "      <td>characters</td>\n",
              "      <td>plot</td>\n",
              "      <td>characters</td>\n",
              "      <td>story</td>\n",
              "      <td>movies</td>\n",
              "      <td>characters</td>\n",
              "      <td>boring</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>comedy</td>\n",
              "      <td>dull</td>\n",
              "      <td>story</td>\n",
              "      <td>ultimately</td>\n",
              "      <td>films</td>\n",
              "      <td>films</td>\n",
              "      <td>movies</td>\n",
              "      <td>bad</td>\n",
              "      <td>story</td>\n",
              "      <td>tale</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>director</td>\n",
              "      <td>pretentious</td>\n",
              "      <td>films</td>\n",
              "      <td>feels</td>\n",
              "      <td>bad</td>\n",
              "      <td>movies</td>\n",
              "      <td>comedy</td>\n",
              "      <td>minutes</td>\n",
              "      <td>feels</td>\n",
              "      <td>bibbidybobbidibland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>plot</td>\n",
              "      <td>mess</td>\n",
              "      <td>love</td>\n",
              "      <td>director</td>\n",
              "      <td>minutes</td>\n",
              "      <td>comedy</td>\n",
              "      <td>time</td>\n",
              "      <td>hard</td>\n",
              "      <td>plot</td>\n",
              "      <td>dull</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ultimately</td>\n",
              "      <td>stupid</td>\n",
              "      <td>life</td>\n",
              "      <td>script</td>\n",
              "      <td>director</td>\n",
              "      <td>plot</td>\n",
              "      <td>hard</td>\n",
              "      <td>characters</td>\n",
              "      <td>films</td>\n",
              "      <td>predictable</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      topic 0      topic 1  ...     topic 8              topic 9\n",
              "0       movie        movie  ...       movie                movie\n",
              "1        film         film  ...        film                 film\n",
              "2         bad       comedy  ...         bad                story\n",
              "3       story         lame  ...      action               comedy\n",
              "4        time          bad  ...      comedy                  bad\n",
              "5       films        silly  ...  characters               boring\n",
              "6      comedy         dull  ...       story                 tale\n",
              "7    director  pretentious  ...       feels  bibbidybobbidibland\n",
              "8        plot         mess  ...        plot                 dull\n",
              "9  ultimately       stupid  ...       films          predictable\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}